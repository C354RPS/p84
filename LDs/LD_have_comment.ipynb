{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a Comments - Consolida Lista de Documentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl.worksheet._reader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados Básicos (MESMO DO CÓDIGO controle.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dados_basicos = {\n",
    "'PKG_DESCRIPTION': ['API 610 CENTRIFUGAL PUMPS', 'DECK TROLLEY', 'FLARE SYSTEM', 'FRESH WATER CHLORINATION UNIT', 'NITROGEN GENERATOR UNITS', 'NON-API 610 CENTRIFUGAL PUMPS', 'PIG LAUNCHERS AND RECEIVERS', 'PROGRESSIVE CAVITY PUMPS', 'RECIPROCATING PUMPS', 'SEA WATER ELECTROCHLORINATION UNIT', 'SEA WATER LIFT PUMP', 'SHELL AND TUBE HEAT EXCHANGERS (Himile)', 'SHELL AND TUBE HEAT EXCHANGERS (Asvotec)', 'WATER INJECTION PUMPS', 'FRESH WATER MAKER FOR OIL DILUTION', 'PRINTED CIRCUIT HEAT EXCHANGER', 'OFFSHORE CRANE'],\n",
    "'Discipline': ['ROE', 'ROE', 'ROE', 'STATIC', 'ROE', 'ROE', 'STATIC', 'ROE', 'ROE', 'STATIC', 'ROE', 'STATIC', 'STATIC', 'ROE', 'STATIC', 'STATIC', 'ROE'],\n",
    "'Critical': ['', '', 'Critical', '', '', '', '', '', 'Critical', '', 'Super Critical', '', '', 'Super Critical', '', 'Critical', 'Critical'],\n",
    "'System': ['1200,1223,5124,5125,5331,5115', '5266', '5412', '5122', '5241', '120N,1200,5115,5124,5125', '1244,1231,1210', '5412', '5115,5133', '5121', '5111', '1223', '1200,1223,1251,1350,5125,5135,5147,1225,1231', '1251', '5122', '1231,1252,1254', '5266'],\n",
    "'Trigram': ['KD9', 'x', 'x', 'D5A', 'GK1', 'KD9', 'AP5', 'NZA', 'KFQ', 'D5A', 'FM2', 'HM9', 'ATI', 'SJA', 'HE1', 'HLT', 'ND1'],\n",
    "'Vendor': ['KSB BRASIL LTDA', '', '', 'DE NORA DO BRASIL LTDA', 'GARDNER DENVER KOREA', 'KSB BRASIL LTDA', 'APPLIED ENGINEERING PTE LTD', 'Netzsch Asia Pacific Pte Ltd', 'PERONI POMPE SPA', 'DE NORA DO BRASIL LTDA', 'FRAMO AS', 'HIMILE MECHANICAL MANUFACTURING (SHANDONG) CO., LTD', 'ASVOTEC TERMOINDUSTRIAL LTDA', 'SULZER BRASIL SA', 'HITACHI AQUA TECH (HAQT)E1', 'HEATRIC DIVISION OF MEGGITT (UK) LIMITED', 'NATIONAL OILWELL VARCO NORWAY AS'],\n",
    "'LD_Number': ['I-LD-3010.2S-1200-940-KD9-001_0','','','I-LD-3010.2S-5122-940-D5A-001_A','I-LD-3010.2S-5241-940-GK1-001_A','I-LD-3010.2S-120N-940-KD9-001_0','I-LD-3010.2S-1210-911-AP5-001_0','I-LD-3010.2S-5412-911-NZA-001_0','I-LD-3010.2S-1200-911-KFQ-001_0','I-LD-3010.2S-5121-940-D5A-001_0','I-LD-3010.2S-5111-940-FM2-001_A','I-LD-3010.2S-1223-940-HM9-001_A','I-LD-3010.2S-1200-940-ATI-001_0','I-LD-3010.2S-1251-911-SJA-001_B','I-LD-3010.2S-5122-940-HE1-101_0','','I-LD-3010.2S-5266-940-ND1-001_0'],\n",
    "'MR_Number': ['I-RM-3010.2S-1200-311-S2N-002', 'I-RM-3010.2S-5266-620-S2N-001', 'I-RM-3010.2S-5412-583-S2N-001', 'I-RM-3010.2S-5122-560-S2N-042', 'I-RM-3010.2S-5241-470-S2N-001', 'I-RM-3010.2S-1200-311-S2N-001', 'I-RM-3010.2S-1210-296-S2N-001', 'I-RM-3010.2S-1200-313-S2N-001', 'I-RM-3010.2S-5133-312-S2N-001', 'I-RM-3010.2S-5121-560-S2N-040', 'I-RM-3010.2S-5111-311-S2N-001', 'I-RM-3010.2S-1200-451-S2N-011', 'I-RM-3010.2S-1200-451-S2N-011', 'I-RM-3010.2S-1251-311-S2N-001', 'I-RM-3010.2S-5122-580-S2N-040', 'I-RM-3010.2S-1200-459-S2N-032', 'I-RM-3010.2S-5266-631-S2N-001'],\n",
    "'TBE_Number': ['I-PT-3010.2S-1200-311-S2N-002', 'I-PT-3010.2S-5266-620-S2N-001', 'I-PT-3010.2S-5412-583-S2N-001', 'I-PT-3010.2S-5122-560-S2N-042', 'I-PT-3010.2S-5241-470-S2N-001, I-PT-3010.2S-5241-470-S2N-002', 'I-PT-3010.2S-1200-311-S2N-001', 'I-PT-3010.2S-1210-296-S2N-018', 'I-PT-3010.2S-1200-313-S2N-001', 'I-PT-3010.2S-5133-312-S2N-001', 'I-PT-3010.2S-5121-560-S2N-040', 'I-PT-3010.2S-5111-311-S2N-001', 'I-PT-3010.2S-1200-451-S2N-011', 'I-PT-3010.2S-1200-451-S2N-011', 'I-PT-3010.2S-1251-311-S2N-001', 'I-PT-3010.2S-5122-580-S2N-040', 'I-PT-3010.2S-1200-459-S2N-032', 'I-PT-3010.2S-5266-631-S2N-001'],\n",
    "'KOM_DATE': ['', '', '', '10/03/2025', '18/02/2025', '', '24/03/2025', '', '', '07/03/2025', '17-18/02/2025', '21/03/2025', '19/03/2025', '21-22/10/2025', '25/03/2025', '','24/03/2025'],\n",
    "'PIM_DATE': ['', '', '', '', '01-02/04/2025', '', 's 08/2025', '', '', '', '', '', '', '17/01/2025', '', '', ''],\n",
    "'DR_MOTORS_DATE': ['', '', '', '', '', '', '', '', '', '', '', '', '', '17/03/2025', '', '',''],\n",
    "'PLAN_DELIVERY_DATE': ['', '', '', '28/02/2026', '30/10/2025 & 28/02/2026', '', '', '', '', '28/02/2026', '01/01/2027', '15/03/2026', '15/03/2026', '03/04/2026', '', '',''],\n",
    "}\n",
    "\n",
    "df_db = pd.DataFrame(data_dados_basicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renomeia Colunas Duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renomear_colunas_revisao(df):\n",
    "    \"\"\"\n",
    "    Renomeia colunas duplicadas em um DataFrame usando prefixos de revisão (rev0_, revA_, revB_, etc.)\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (pandas.DataFrame): DataFrame com possíveis nomes de colunas duplicados\n",
    "        \n",
    "    Retorna:\n",
    "        pandas.DataFrame: DataFrame com nomes de colunas únicos\n",
    "    \"\"\"\n",
    "    # Cria uma cópia do DataFrame para não modificar o original\n",
    "    df_renomeado = df.copy()\n",
    "    \n",
    "    # Identificando as colunas duplicadas e suas posições\n",
    "    posicoes = {}\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if col not in posicoes:\n",
    "            posicoes[col] = []\n",
    "        posicoes[col].append(i)\n",
    "    \n",
    "    # Lista de prefixos de revisão\n",
    "    prefixos_revisao = ['rev0_', 'revA_', 'revB_', 'revC_', 'revD_', 'revE_', \n",
    "                         'revF_', 'revG_', 'revH_', 'revI_', 'revJ_']\n",
    "    \n",
    "    # Criando novos nomes com os prefixos de revisão\n",
    "    novos_nomes = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if len(posicoes[col]) > 1:\n",
    "            # Encontra o índice desta ocorrência na lista de posições\n",
    "            idx = posicoes[col].index(i)\n",
    "            if idx < len(prefixos_revisao):\n",
    "                novos_nomes.append(f'{prefixos_revisao[idx]}{col}')\n",
    "            else:\n",
    "                # Caso tenha mais revisões que prefixos disponíveis\n",
    "                novos_nomes.append(f'rev{idx}_{col}')\n",
    "        else:\n",
    "            # Colunas sem duplicatas mantêm o nome original\n",
    "            novos_nomes.append(col)\n",
    "    \n",
    "    # Atribui os novos nomes às colunas\n",
    "    df_renomeado.columns = novos_nomes\n",
    "    \n",
    "    return df_renomeado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD Number: I-LD-3010.2S-1200-940-KD9-001_0, processando arquivos do pacote: API 610 CENTRIFUGAL PUMPS...\n",
      "LD Number vazia. Pulando...\n",
      "LD Number vazia. Pulando...\n",
      "LD Number: I-LD-3010.2S-5122-940-D5A-001_A, processando arquivos do pacote: FRESH WATER CHLORINATION UNIT...\n",
      "LD Number: I-LD-3010.2S-5241-940-GK1-001_A, processando arquivos do pacote: NITROGEN GENERATOR UNITS...\n",
      "LD Number: I-LD-3010.2S-120N-940-KD9-001_0, processando arquivos do pacote: NON-API 610 CENTRIFUGAL PUMPS...\n",
      "LD Number: I-LD-3010.2S-1210-911-AP5-001_0, processando arquivos do pacote: PIG LAUNCHERS AND RECEIVERS...\n",
      "LD Number: I-LD-3010.2S-5412-911-NZA-001_0, processando arquivos do pacote: PROGRESSIVE CAVITY PUMPS...\n",
      "LD Number: I-LD-3010.2S-1200-911-KFQ-001_0, processando arquivos do pacote: RECIPROCATING PUMPS...\n",
      "LD Number: I-LD-3010.2S-5121-940-D5A-001_0, processando arquivos do pacote: SEA WATER ELECTROCHLORINATION UNIT...\n",
      "LD Number: I-LD-3010.2S-5111-940-FM2-001_A, processando arquivos do pacote: SEA WATER LIFT PUMP...\n",
      "LD Number: I-LD-3010.2S-1223-940-HM9-001_A, processando arquivos do pacote: SHELL AND TUBE HEAT EXCHANGERS (Himile)...\n",
      "LD Number: I-LD-3010.2S-1200-940-ATI-001_0, processando arquivos do pacote: SHELL AND TUBE HEAT EXCHANGERS (Asvotec)...\n",
      "LD Number: I-LD-3010.2S-1251-911-SJA-001_B, processando arquivos do pacote: WATER INJECTION PUMPS...\n",
      "LD Number: I-LD-3010.2S-5122-940-HE1-101_0, processando arquivos do pacote: FRESH WATER MAKER FOR OIL DILUTION...\n",
      "LD Number vazia. Pulando...\n",
      "LD Number: I-LD-3010.2S-5266-940-ND1-001_0, processando arquivos do pacote: OFFSHORE CRANE...\n"
     ]
    }
   ],
   "source": [
    "# Caminho base para os arquivos\n",
    "base_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\files\\\\'\n",
    "# Arquivo do relatório Integra\n",
    "integra_report = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\RE-General Query - Technical Engineering Documents.xlsx'\n",
    "\n",
    "# Dataframe para armazenar todos os resultados\n",
    "df_LD_final = pd.DataFrame()\n",
    "\n",
    "df_integra = pd.read_excel(integra_report, sheet_name=\"DADOS\")\n",
    "\n",
    "# Iterar sobre cada linha do dataframe\n",
    "for _, row in df_db.iterrows():\n",
    "    ld_number = row['LD_Number']\n",
    "    package = row['PKG_DESCRIPTION']\n",
    "\n",
    "    if ld_number:\n",
    "        print(f'LD Number: {ld_number}, processando arquivos do pacote: {package}...')\n",
    "        ld_files = glob.glob(f'{base_path}{ld_number}*.xlsx')\n",
    "        \n",
    "        if len(ld_files) < 2:\n",
    "            print(f\"Não foram encontrados pelo menos 2 arquivos para o pacote {package}\")\n",
    "            continue\n",
    "\n",
    "        ld_file_comments = [f for f in ld_files if '_comments' in f][0]\n",
    "\n",
    "        # Carregar os dataframes \n",
    "        df_LD_comments = pd.read_excel(ld_file_comments, sheet_name=\"VDRL\")\n",
    "    \n",
    "        df_LD_comments.columns = df_LD_comments.iloc[6]\n",
    "        df_LD_comments = df_LD_comments.iloc[7:]\n",
    "    \n",
    "        df_LD_comments = renomear_colunas_revisao(df_LD_comments)\n",
    "\n",
    "        df_LD_comments.rename(columns = {\"Have comment?\":\"Have_comment\", \"CLIENT DOCUMENT NUMBER\":\"CLIENT_DOCUMENT\",\n",
    "                                \"VENDOR DOCUMENT NUMBER\":\"VENDOR_DOCUMENT\",\n",
    "                                \"DOCUMENT TITLE\": \"DOCUMENT_TITLE\"}, inplace = True)\n",
    "        \n",
    "        df_LD_comments = df_LD_comments.dropna(subset=['DOCUMENT_TITLE'])\n",
    "        # Remover espaços em branco e converter para maiúsculas\n",
    "        df_LD_comments['DOCUMENT_TITLE'] = df_LD_comments['DOCUMENT_TITLE'].apply(lambda x: x.strip().upper())\n",
    "    \n",
    "        df_integra.rename(columns = {\"CREATION DATE\":\"CREATION_DATE\"}, inplace = True)\n",
    "\n",
    "        colunas_para_manter = ['Have_comment', 'Discipline', 'CLIENT_DOCUMENT', 'VENDOR_DOCUMENT', 'DOCUMENT_TITLE', 'ORIGINATOR ', 'rev0_PLANNED DATE','rev0_ACTUAL DATE',\n",
    "                                'rev0_RETURNED DATE','rev0_RETURNED CODE','revA_PLANNED DATE','revA_ACTUAL DATE','revA_RETURNED DATE','revA_RETURNED CODE', \n",
    "                                'revB_PLANNED DATE','revB_ACTUAL DATE','revB_RETURNED DATE','revB_RETURNED CODE']\n",
    "        df_LD_comments.drop(columns=[col for col in df_LD_comments.columns if col not in colunas_para_manter], inplace = True)\n",
    "        df_LD_comments.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "        df_LD_comments = df_LD_comments.iloc[:, :-2] \n",
    "\n",
    "        # Get only the needed columns from df_integra\n",
    "        code_mapping = df_integra[['CODE', 'TITLE', 'CREATION_DATE']].copy()\n",
    "        # Create a dictionary for faster lookups\n",
    "        code_dict = dict(zip(code_mapping['CODE'], zip(code_mapping['TITLE'], code_mapping['CREATION_DATE'])))\n",
    "        # Create new columns all at once using map\n",
    "        df_LD_comments['CODE'] = df_LD_comments['CLIENT_DOCUMENT'].map(lambda x: x if x in code_dict else None)\n",
    "        df_LD_comments['TITLE'] = df_LD_comments['CLIENT_DOCUMENT'].map(lambda x: code_dict.get(x, (None,None))[0])\n",
    "        df_LD_comments['CREATION_DATE'] = df_LD_comments['CLIENT_DOCUMENT'].map(lambda x: code_dict.get(x, (None,None))[1])\n",
    "        df_LD_comments['PKG_DESCRIPTION'] = package\n",
    "\n",
    "        # Adicionar ao dataframe final\n",
    "        df_LD_final = pd.concat([df_LD_final, df_LD_comments], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"LD Number vazia. Pulando...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria uma coluna 'PLANNED DATE' com os valores mais atualizados da LD e Reordena a LD_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualizar_planned_date_coalesce(df_LD):\n",
    "    \"\"\"\n",
    "    Versão usando coalesce para criar uma coluna 'PLANNED DATE' com os valores mais atualizados,\n",
    "    priorizando a revisão mais recente com valor válido.\n",
    "    \"\"\"\n",
    "    # Cria uma cópia do DataFrame\n",
    "    df = df_LD.copy()\n",
    "    \n",
    "    # Identifica as colunas de PLANNED DATE em ordem de prioridade\n",
    "    colunas_planned_date = [col for col in df.columns if 'PLANNED DATE' in col]\n",
    "    colunas_planned_date.sort(reverse=True)  # Coloca em ordem: revB, revA, rev0\n",
    "    \n",
    "    # Usa o método coalesce do pandas para pegar o primeiro valor não-NaN\n",
    "    df['PLANNED_DATE'] = df[colunas_planned_date].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "    # Definimos a nova ordem das colunas em uma lista\n",
    "    new_order = ['PKG_DESCRIPTION','CLIENT_DOCUMENT','VENDOR_DOCUMENT','DOCUMENT_TITLE','rev0_PLANNED DATE','rev0_ACTUAL DATE','rev0_RETURNED DATE','rev0_RETURNED CODE','revA_PLANNED DATE','revA_ACTUAL DATE','revA_RETURNED DATE','revA_RETURNED CODE','revB_PLANNED DATE','revB_ACTUAL DATE','CODE','TITLE','CREATION_DATE','PLANNED_DATE','Have_comment','Discipline',]\n",
    "\n",
    "    return df[new_order]\n",
    "\n",
    "df_LD_final = atualizar_planned_date_coalesce(df_LD_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista os documentos que não batem a descrição e grava no arquivo \"have_a_comment_mismatches.xlsx\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with mismatching titles:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>CLIENT_DOCUMENT</th>\n",
       "      <th>DOCUMENT_TITLE</th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-021</td>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-021</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL &amp; S...</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-041</td>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-041</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL &amp; S...</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-061</td>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-061</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL &amp; S...</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-081</td>\n",
       "      <td>I-PR-3010.2S-1223-393-KD9-081</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL &amp; S...</td>\n",
       "      <td>INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>I-LD-3010.2S-5122-940-D5A-001</td>\n",
       "      <td>I-LD-3010.2S-5122-940-D5A-001</td>\n",
       "      <td>Z-5122001 - FRESH WATER CHLORINATION UNIT - VE...</td>\n",
       "      <td>FRESH WATER CHLORINATION UNIT (Z-5122001) VEND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>I-DE-3010.2S-5266-191-ND1-002</td>\n",
       "      <td>I-DE-3010.2S-5266-191-ND1-002</td>\n",
       "      <td>GENERAL ARRANGEMENT DRAWING PEDESTAL ADAPTER_F...</td>\n",
       "      <td>GENERAL ARRANGEMENT DRAWING PEDESTAL ADAPTER-F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2891</th>\n",
       "      <td>I-DE-3010.2S-5266-942-ND1-001</td>\n",
       "      <td>I-DE-3010.2S-5266-942-ND1-001</td>\n",
       "      <td>GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE_AFT...</td>\n",
       "      <td>GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE-AFT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2892</th>\n",
       "      <td>I-DE-3010.2S-5266-942-ND1-002</td>\n",
       "      <td>I-DE-3010.2S-5266-942-ND1-002</td>\n",
       "      <td>GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE_FWD...</td>\n",
       "      <td>GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE-FWD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>I-DE-3010.2S-5266-942-ND1-003</td>\n",
       "      <td>I-DE-3010.2S-5266-942-ND1-003</td>\n",
       "      <td>3D MODEL (SIMPLIFIED 3D MODEL IN STP/STEP FORM...</td>\n",
       "      <td>3D MODEL (SIMPLIFIED 3D MODEL IN STP/STEP FORM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2922</th>\n",
       "      <td>I-PR-3010.2S-5266-98F-ND1-001</td>\n",
       "      <td>I-PR-3010.2S-5266-98F-ND1-001</td>\n",
       "      <td>HSE PLAN/PROGRAM PEDESTAL CRANE</td>\n",
       "      <td>HSE PLAN-PROGRAM PEDESTAL CRANE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               CODE                CLIENT_DOCUMENT  \\\n",
       "46    I-PR-3010.2S-1223-393-KD9-021  I-PR-3010.2S-1223-393-KD9-021   \n",
       "96    I-PR-3010.2S-1223-393-KD9-041  I-PR-3010.2S-1223-393-KD9-041   \n",
       "147   I-PR-3010.2S-1223-393-KD9-061  I-PR-3010.2S-1223-393-KD9-061   \n",
       "197   I-PR-3010.2S-1223-393-KD9-081  I-PR-3010.2S-1223-393-KD9-081   \n",
       "474   I-LD-3010.2S-5122-940-D5A-001  I-LD-3010.2S-5122-940-D5A-001   \n",
       "...                             ...                            ...   \n",
       "2834  I-DE-3010.2S-5266-191-ND1-002  I-DE-3010.2S-5266-191-ND1-002   \n",
       "2891  I-DE-3010.2S-5266-942-ND1-001  I-DE-3010.2S-5266-942-ND1-001   \n",
       "2892  I-DE-3010.2S-5266-942-ND1-002  I-DE-3010.2S-5266-942-ND1-002   \n",
       "2893  I-DE-3010.2S-5266-942-ND1-003  I-DE-3010.2S-5266-942-ND1-003   \n",
       "2922  I-PR-3010.2S-5266-98F-ND1-001  I-PR-3010.2S-5266-98F-ND1-001   \n",
       "\n",
       "                                         DOCUMENT_TITLE  \\\n",
       "46    INSPECTION AND TEST PLAN - MECHANICAL SEAL & S...   \n",
       "96    INSPECTION AND TEST PLAN - MECHANICAL SEAL & S...   \n",
       "147   INSPECTION AND TEST PLAN - MECHANICAL SEAL & S...   \n",
       "197   INSPECTION AND TEST PLAN - MECHANICAL SEAL & S...   \n",
       "474   Z-5122001 - FRESH WATER CHLORINATION UNIT - VE...   \n",
       "...                                                 ...   \n",
       "2834  GENERAL ARRANGEMENT DRAWING PEDESTAL ADAPTER_F...   \n",
       "2891  GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE_AFT...   \n",
       "2892  GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE_FWD...   \n",
       "2893  3D MODEL (SIMPLIFIED 3D MODEL IN STP/STEP FORM...   \n",
       "2922                    HSE PLAN/PROGRAM PEDESTAL CRANE   \n",
       "\n",
       "                                                  TITLE  \n",
       "46    INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...  \n",
       "96    INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...  \n",
       "147   INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...  \n",
       "197   INSPECTION AND TEST PLAN - MECHANICAL SEAL AND...  \n",
       "474   FRESH WATER CHLORINATION UNIT (Z-5122001) VEND...  \n",
       "...                                                 ...  \n",
       "2834  GENERAL ARRANGEMENT DRAWING PEDESTAL ADAPTER-F...  \n",
       "2891  GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE-AFT...  \n",
       "2892  GENERAL ARRANGEMENT DRAWING PEDESTAL CRANE-FWD...  \n",
       "2893  3D MODEL (SIMPLIFIED 3D MODEL IN STP/STEP FORM...  \n",
       "2922                    HSE PLAN-PROGRAM PEDESTAL CRANE  \n",
       "\n",
       "[156 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\have_a_comment_mismatches.xlsx salvo em:  c:\\Users\\ELXY\\Documents\\Codigos\\Python\\P84_85\\LDs\n"
     ]
    }
   ],
   "source": [
    "# Create mask for non-matching titles\n",
    "mask = (df_LD_final['DOCUMENT_TITLE'] != df_LD_final['TITLE']) & (~df_LD_final['TITLE'].isna())\n",
    "\n",
    "# Get mismatched documents\n",
    "mismatches = df_LD_final[mask][['CODE', 'CLIENT_DOCUMENT', 'DOCUMENT_TITLE', 'TITLE']]\n",
    "\n",
    "print(\"Documents with mismatching titles:\")\n",
    "display(mismatches)\n",
    "\n",
    "# Salvar o DataFrame final\n",
    "output_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\have_a_comment_mismatches.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    mismatches.to_excel(writer, sheet_name='mismatches', index=False)\n",
    "\n",
    "print(f'Arquivo {output_file} salvo em: ', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar o DataFrame final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\LD_consolidada_have_a_comment.xlsx salvo em:  c:\\Users\\ELXY\\Documents\\Codigos\\Python\\P84_85\\LDs\n"
     ]
    }
   ],
   "source": [
    "# Salvar o DataFrame final\n",
    "output_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_LD_final.to_excel(writer, sheet_name='VDRL', index=False)\n",
    "\n",
    "print(f'Arquivo {output_file} salvo em: ', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega Have a Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "intput_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "df_LD_final = pd.read_excel(intput_file, sheet_name=\"VDRL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolida todas as VDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo existente C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\vda.xlsx foi excluído com sucesso.\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA ATI 02.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA D5A 33.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA FM2 14.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA GK1 20.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA HM9 06.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA ND1 01.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA-75 Docs Sulzer.xlsx\n",
      "Ignorando arquivo temporário: ~$vda.xlsx\n",
      "Arquivo consolidado salvo em: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\vda.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho da pasta contendo os arquivos VDA \n",
    "vda_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\'\n",
    "\n",
    "# Caminho do arquivo consolidado\n",
    "output_path = os.path.join(vda_path, 'vda.xlsx')\n",
    "\n",
    "# Verificar se o arquivo consolidado já existe e excluí-lo\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"Arquivo existente {output_path} foi excluído com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao excluir o arquivo {output_path}: {e}\")\n",
    "        print(\"O arquivo pode estar aberto em outro programa. Feche-o e tente novamente.\")\n",
    "        exit()  # Encerrar o script se não conseguir excluir o arquivo\n",
    "\n",
    "# Lista para armazenar todos os dataframes \n",
    "all_dfs = [] \n",
    "\n",
    "# Iterar sobre todos os arquivos Excel na pasta \n",
    "for filename in os.listdir(vda_path): \n",
    "    # Ignorar arquivos temporários/ocultos que começam com ~$ ou .\n",
    "    if filename.startswith('~$') or filename.startswith('.'):\n",
    "        print(f\"Ignorando arquivo temporário: {filename}\")\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.xlsx'): \n",
    "        file_path = os.path.join(vda_path, filename) \n",
    "        print(f\"Lendo arquivo: {file_path}\")\n",
    "        try:\n",
    "            # Especificar o engine como 'openpyxl' para arquivos .xlsx\n",
    "            df = pd.read_excel(file_path, engine='openpyxl') \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_path}: {e}\")\n",
    "            \n",
    "    elif filename.endswith('.xls'):\n",
    "        file_path = os.path.join(vda_path, filename) \n",
    "        print(f\"Lendo arquivo: {file_path}\")\n",
    "        try:\n",
    "            # Especificar o engine como 'xlrd' para arquivos .xls\n",
    "            df = pd.read_excel(file_path, engine='xlrd') \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_path}: {e}\")\n",
    "\n",
    "# Verificar se encontramos algum arquivo\n",
    "if all_dfs:\n",
    "    # Concatenar todos os dataframes \n",
    "    consolidated_df = pd.concat(all_dfs, ignore_index=True) \n",
    "    # Filter the dataframe to keep only system 1223 for HM9 trigram, and keep all data for other trigrams\n",
    "    consolidated_df = consolidated_df[\n",
    "        ((consolidated_df['Reference Document'].str.contains('-HM9-', na=False)) & \n",
    "        (consolidated_df['Reference Document'].str.contains('-1223-', na=False))) |\n",
    "        (~consolidated_df['Reference Document'].str.contains('-HM9-', na=False))\n",
    "]\n",
    "    # Salvar o dataframe consolidado e filtrado como um novo arquivo Excel \n",
    "    consolidated_df.to_excel(output_path, index=False) \n",
    "    print(f'Arquivo consolidado salvo em: {output_path}')\n",
    "else:\n",
    "    print(\"Nenhum arquivo Excel válido encontrado no diretório.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaciona o documento com a to-do-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo To Do List atualizado e salvo em: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\To Do List.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Caminhos dos arquivos \n",
    "todo_list_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\To Do List.xlsx'\n",
    "vda_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\vda.xlsx'\n",
    "\n",
    "# Leitura dos arquivos \n",
    "vda_df = pd.read_excel(vda_path)\n",
    "\n",
    "# Tentar ler o arquivo To Do List com diferentes métodos \n",
    "try: \n",
    "    # Primeiro, tente ler normalmente \n",
    "    todo_df = pd.read_excel(todo_list_path, engine='openpyxl')\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo normalmente: {e}\")\n",
    "    try:\n",
    "        # Se falhar, tente ler apenas os dados \n",
    "        wb = load_workbook(filename=todo_list_path, read_only=True, data_only=True)\n",
    "        ws = wb.active \n",
    "        data = ws.values \n",
    "        cols = next(data)[0:] \n",
    "        todo_df = pd.DataFrame(data, columns=cols)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo com openpyxl: {e}\")\n",
    "        # Se ambos falharem, crie um DataFrame vazio \n",
    "        todo_df = pd.DataFrame(columns=['Related Object'])\n",
    "\n",
    "# Verificar se a coluna 'Related Object' existe\n",
    "if 'Related Object' not in todo_df.columns: \n",
    "    print(\"A coluna 'Related Object' não foi encontrada. Adicionando uma coluna vazia.\")\n",
    "    todo_df['Related Object'] = ''\n",
    "\n",
    "# Resto do código permanece o mesmo \n",
    "# Merge do vda_df com df_LD_final \n",
    "merged_df = pd.merge(vda_df, df_LD_final, left_on='Reference Document', right_on='CLIENT_DOCUMENT', how='left')\n",
    "\n",
    "# Função para criar a string de informações \n",
    "def create_info_string(row):\n",
    "    if pd.notna(row['CLIENT_DOCUMENT']):\n",
    "        # Obter a data atual no formato desejado\n",
    "        current_date = datetime.now().strftime(\"%d-%B-%Y\")\n",
    "        # Limpar e padronizar o valor de Have_comment\n",
    "        have_comment = str(row['Have_comment']).strip().upper()\n",
    "\n",
    "        if have_comment == 'YES':\n",
    "            return f\"DOCUMENT: {row['CLIENT_DOCUMENT']} {row['Title']}, Comment: {row['Have_comment']}, Discipline: {row['Discipline']}, Date: {current_date}\"\n",
    "        else:\n",
    "            return f\"DOCUMENT: {row['CLIENT_DOCUMENT']} {row['Title']}, Comment: NO, Date: {current_date}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Aplicar a função para criar a coluna de informações \n",
    "merged_df['Info'] = merged_df.apply(create_info_string, axis=1)\n",
    "\n",
    "# Criar um dicionário de Code para Info\n",
    "code_to_info = dict(zip(merged_df['Code'], merged_df['Info']))\n",
    "\n",
    "# Função para preencher a coluna Additional Info\n",
    "def fill_additional_info(code):\n",
    "    return code_to_info.get(code, '')\n",
    "\n",
    "# Adicionar a nova coluna Additional Info no todo_df \n",
    "todo_df['Additional Info'] = todo_df['Related Object'].apply(fill_additional_info)\n",
    "\n",
    "# Salvar o arquivo atualizado\n",
    "todo_df.to_excel(todo_list_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f'Arquivo To Do List atualizado e salvo em: {todo_list_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
