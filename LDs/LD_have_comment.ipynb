{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a Comments - Consolida Lista de Documentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl.worksheet._reader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega Dados Básicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct path to the Excel file\n",
    "excel_filename = 'C:/Users/ELXY/Documents/Codigos/Python/P84_85/DadosBasicos/dados_basicos.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "df_db = pd.read_excel(excel_filename, sheet_name=0)  # sheet_name=0 reads the first sheet\n",
    "df_db = df_db.fillna('')\n",
    "# display(df_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renomeia Colunas Duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renomear_colunas_revisao(df):\n",
    "    \"\"\"\n",
    "    Renomeia colunas duplicadas em um DataFrame usando prefixos de revisão (rev0_, revA_, revB_, etc.)\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (pandas.DataFrame): DataFrame com possíveis nomes de colunas duplicados\n",
    "        \n",
    "    Retorna:\n",
    "        pandas.DataFrame: DataFrame com nomes de colunas únicos\n",
    "    \"\"\"\n",
    "    # Cria uma cópia do DataFrame para não modificar o original\n",
    "    df_renomeado = df.copy()\n",
    "    \n",
    "    # Identificando as colunas duplicadas e suas posições\n",
    "    posicoes = {}\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if col not in posicoes:\n",
    "            posicoes[col] = []\n",
    "        posicoes[col].append(i)\n",
    "    \n",
    "    # Lista de prefixos de revisão\n",
    "    prefixos_revisao = ['rev0_', 'revA_', 'revB_', 'revC_', 'revD_', 'revE_', \n",
    "                         'revF_', 'revG_', 'revH_', 'revI_', 'revJ_']\n",
    "    \n",
    "    # Criando novos nomes com os prefixos de revisão\n",
    "    novos_nomes = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if len(posicoes[col]) > 1:\n",
    "            # Encontra o índice desta ocorrência na lista de posições\n",
    "            idx = posicoes[col].index(i)\n",
    "            if idx < len(prefixos_revisao):\n",
    "                novos_nomes.append(f'{prefixos_revisao[idx]}{col}')\n",
    "            else:\n",
    "                # Caso tenha mais revisões que prefixos disponíveis\n",
    "                novos_nomes.append(f'rev{idx}_{col}')\n",
    "        else:\n",
    "            # Colunas sem duplicatas mantêm o nome original\n",
    "            novos_nomes.append(col)\n",
    "    \n",
    "    # Atribui os novos nomes às colunas\n",
    "    df_renomeado.columns = novos_nomes\n",
    "    \n",
    "    return df_renomeado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD Number: I-LD-3010.2S-1200-940-KD9-001_0, processando arquivos do pacote: API 610 CENTRIFUGAL PUMPS...\n",
      "LD Number vazia. Pulando...\n",
      "LD Number vazia. Pulando...\n",
      "LD Number: I-LD-3010.2S-5122-940-D5A-001_B, processando arquivos do pacote: FRESH WATER CHLORINATION UNIT...\n",
      "LD Number: I-LD-3010.2S-5241-940-GK1-001_A, processando arquivos do pacote: NITROGEN GENERATOR UNITS...\n",
      "LD Number: I-LD-3010.2S-120N-940-KD9-001_0, processando arquivos do pacote: NON-API 610 CENTRIFUGAL PUMPS...\n",
      "LD Number: I-LD-3010.2S-1210-911-AP5-001_A, processando arquivos do pacote: PIG LAUNCHERS AND RECEIVERS...\n",
      "LD Number: I-LD-3010.2S-5412-911-NZA-001_A, processando arquivos do pacote: PROGRESSIVE CAVITY PUMPS...\n",
      "LD Number: I-LD-3010.2S-5000-940-KFQ-001_0, processando arquivos do pacote: RECIPROCATING PUMPS...\n",
      "LD Number: I-LD-3010.2S-5121-940-D5A-001_A, processando arquivos do pacote: SEA WATER ELECTROCHLORINATION UNIT...\n",
      "LD Number: I-LD-3010.2S-5111-940-FM2-001_B, processando arquivos do pacote: SEA WATER LIFT PUMP...\n",
      "LD Number: I-LD-3010.2S-1223-940-HM9-001_B, processando arquivos do pacote: SHELL AND TUBE HEAT EXCHANGERS (Himile)...\n",
      "LD Number: I-LD-3010.2S-1200-940-ATI-001_0, processando arquivos do pacote: SHELL AND TUBE HEAT EXCHANGERS (Asvotec)...\n",
      "LD Number: I-LD-3010.2S-1251-911-SJA-001_C, processando arquivos do pacote: WATER INJECTION PUMPS...\n",
      "LD Number: I-LD-3010.2S-5122-940-HE1-101_A, processando arquivos do pacote: FRESH WATER MAKER FOR OIL DILUTION...\n",
      "LD Number: I-LD-3010.2S-1200-940-HLT-001_0, processando arquivos do pacote: PRINTED CIRCUIT HEAT EXCHANGER...\n",
      "LD Number: I-LD-3010.2S-5266-940-ND1-001_A, processando arquivos do pacote: OFFSHORE CRANE...\n"
     ]
    }
   ],
   "source": [
    "# Caminho base para os arquivos\n",
    "base_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\files\\\\'\n",
    "# Arquivo do relatório Integra\n",
    "integra_report = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\RE-General Query - Technical Engineering Documents.xlsx'\n",
    "\n",
    "# Dataframe para armazenar todos os resultados\n",
    "df_LD_final = pd.DataFrame()\n",
    "\n",
    "df_integra = pd.read_excel(integra_report, sheet_name=\"DADOS\")\n",
    "\n",
    "# Iterar sobre cada linha do dataframe\n",
    "for _, row in df_db.iterrows():\n",
    "    ld_number = row['LD_Number']\n",
    "    package = row['PKG_DESCRIPTION']\n",
    "\n",
    "    if ld_number:\n",
    "        print(f'LD Number: {ld_number}, processando arquivos do pacote: {package}...')\n",
    "        ld_files = glob.glob(f'{base_path}{ld_number}*.xlsx')\n",
    "        \n",
    "        if len(ld_files) < 2:\n",
    "            print(f\"Não foram encontrados pelo menos 2 arquivos para o pacote {package}\")\n",
    "            continue\n",
    "\n",
    "        ld_file_comments = [f for f in ld_files if '_comments' in f][0]\n",
    "\n",
    "        # Carregar os dataframes \n",
    "        df_LD_comments = pd.read_excel(ld_file_comments, sheet_name=\"VDRL\")\n",
    "    \n",
    "        df_LD_comments.columns = df_LD_comments.iloc[6]\n",
    "        df_LD_comments = df_LD_comments.iloc[7:]\n",
    "    \n",
    "        df_LD_comments = renomear_colunas_revisao(df_LD_comments)\n",
    "\n",
    "        df_LD_comments.rename(columns = {\"Have comment?\":\"Comments\", \"CLIENT DOCUMENT NUMBER\":\"CLIENT_DOCUMENT\",\n",
    "                                \"VENDOR DOCUMENT NUMBER\":\"VENDOR_DOCUMENT\",\n",
    "                                \"DOCUMENT TITLE\": \"DOCUMENT_TITLE\"}, inplace = True)\n",
    "        \n",
    "        df_LD_comments = df_LD_comments.dropna(subset=['DOCUMENT_TITLE'])\n",
    "        # Remover espaços em branco e converter para maiúsculas\n",
    "        df_LD_comments['DOCUMENT_TITLE'] = df_LD_comments['DOCUMENT_TITLE'].apply(lambda x: x.strip().upper())\n",
    "    \n",
    "        df_integra.rename(columns = {\"CREATION DATE\":\"CREATION_DATE\"}, inplace = True)\n",
    "\n",
    "        colunas_para_manter = ['Comments', 'Discipline', 'CLIENT_DOCUMENT', 'VENDOR_DOCUMENT', 'DOCUMENT_TITLE', 'ORIGINATOR ', 'rev0_PLANNED DATE','rev0_ACTUAL DATE',\n",
    "                                'rev0_RETURNED DATE','rev0_RETURNED CODE','revA_PLANNED DATE','revA_ACTUAL DATE','revA_RETURNED DATE','revA_RETURNED CODE', \n",
    "                                'revB_PLANNED DATE','revB_ACTUAL DATE','revB_RETURNED DATE','revB_RETURNED CODE']\n",
    "        df_LD_comments.drop(columns=[col for col in df_LD_comments.columns if col not in colunas_para_manter], inplace = True)\n",
    "        df_LD_comments.reset_index(drop=True, inplace = True)\n",
    "    \n",
    "        df_LD_comments = df_LD_comments.iloc[:, :-2] \n",
    "\n",
    "        # Get only the needed columns from df_integra\n",
    "        code_mapping = df_integra[['CODE', 'TITLE', 'CREATION_DATE']].copy()\n",
    "        # Create a dictionary for faster lookups\n",
    "        code_dict = dict(zip(code_mapping['CODE'], zip(code_mapping['TITLE'], code_mapping['CREATION_DATE'])))\n",
    "        # Create new columns all at once using map\n",
    "        df_LD_comments['CODE'] = df_LD_comments['CLIENT_DOCUMENT'].map(lambda x: x if x in code_dict else None)\n",
    "        df_LD_comments['TITLE'] = df_LD_comments['CLIENT_DOCUMENT'].map(lambda x: code_dict.get(x, (None,None))[0])\n",
    "        df_LD_comments['CREATION_DATE'] = df_LD_comments['CLIENT_DOCUMENT'].map(lambda x: code_dict.get(x, (None,None))[1])\n",
    "        df_LD_comments['PKG_DESCRIPTION'] = package\n",
    "\n",
    "        # Adicionar ao dataframe final\n",
    "        df_LD_final = pd.concat([df_LD_final, df_LD_comments], ignore_index=True)\n",
    "    else:\n",
    "        print(f\"LD Number vazia. Pulando...\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria uma coluna 'PLANNED DATE' com os valores mais atualizados da LD e Reordena a LD_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualizar_planned_date_coalesce(df_LD):\n",
    "    \"\"\"\n",
    "    Versão usando coalesce para criar uma coluna 'PLANNED DATE' com os valores mais atualizados,\n",
    "    priorizando a revisão mais recente com valor válido.\n",
    "    \"\"\"\n",
    "    # Cria uma cópia do DataFrame\n",
    "    df = df_LD.copy()\n",
    "    \n",
    "    # Identifica as colunas de PLANNED DATE em ordem de prioridade\n",
    "    colunas_planned_date = [col for col in df.columns if 'PLANNED DATE' in col]\n",
    "    colunas_planned_date.sort(reverse=True)  # Coloca em ordem: revB, revA, rev0\n",
    "    \n",
    "    # Usa o método coalesce do pandas para pegar o primeiro valor não-NaN\n",
    "    df['PLANNED_DATE'] = df[colunas_planned_date].bfill(axis=1).iloc[:, 0]\n",
    "\n",
    "    # Definimos a nova ordem das colunas em uma lista\n",
    "    new_order = ['PKG_DESCRIPTION','CLIENT_DOCUMENT','VENDOR_DOCUMENT','DOCUMENT_TITLE','rev0_PLANNED DATE','rev0_ACTUAL DATE','rev0_RETURNED DATE','rev0_RETURNED CODE','revA_PLANNED DATE','revA_ACTUAL DATE','revA_RETURNED DATE','revA_RETURNED CODE','revB_PLANNED DATE','revB_ACTUAL DATE','CODE','TITLE','CREATION_DATE','PLANNED_DATE','Comments','Discipline']\n",
    "\n",
    "    return df[new_order]\n",
    "\n",
    "df_LD_final = atualizar_planned_date_coalesce(df_LD_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista os documentos que não batem a descrição e grava no arquivo \"have_a_comment_mismatches.xlsx\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with mismatching titles:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODE</th>\n",
       "      <th>CLIENT_DOCUMENT</th>\n",
       "      <th>DOCUMENT_TITLE</th>\n",
       "      <th>TITLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I-PR-3010.2S-1200-974-KD9-001</td>\n",
       "      <td>I-PR-3010.2S-1200-974-KD9-001</td>\n",
       "      <td>PRESSURE TEST PROCEDURES -  API 610 &amp; NON-API ...</td>\n",
       "      <td>PRESSURE TEST PROCEDURES - API 610 AND NON-API...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I-PR-3010.2S-1200-956-KD9-001</td>\n",
       "      <td>I-PR-3010.2S-1200-956-KD9-001</td>\n",
       "      <td>SURFACE PREPARATION AND PAINTING PROCEDURE -  ...</td>\n",
       "      <td>SURFACE PREPARATION AND PAINTING PROCEDURE - A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I-PR-3010.2S-1200-972-KD9-001</td>\n",
       "      <td>I-PR-3010.2S-1200-972-KD9-001</td>\n",
       "      <td>PMI PROCEDURE - API 610 &amp; NON-API CENTRIFUGAL ...</td>\n",
       "      <td>PMI PROCEDURE - API 610 AND NON-API CENTRIFUGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I-PR-3010.2S-1200-972-KD9-002</td>\n",
       "      <td>I-PR-3010.2S-1200-972-KD9-002</td>\n",
       "      <td>NON-DESTRUCTIVE EXAMINATION PROCEDURE - API 61...</td>\n",
       "      <td>NON-DESTRUCTIVE EXAMINATION PROCEDURE - API 61...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-PR-3010.2S-1200-972-KD9-003</td>\n",
       "      <td>I-PR-3010.2S-1200-972-KD9-003</td>\n",
       "      <td>HARDNESS PROCEDURE - API 610&amp; NON-API  CENTRIF...</td>\n",
       "      <td>HARDNESS PROCEDURE - API 610AND NON-API CENTRI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-001</td>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-001</td>\n",
       "      <td>INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...</td>\n",
       "      <td>INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3163</th>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-002</td>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-002</td>\n",
       "      <td>INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...</td>\n",
       "      <td>INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-004</td>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-004</td>\n",
       "      <td>SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS_AFT PED...</td>\n",
       "      <td>SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS-AFT PED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-005</td>\n",
       "      <td>I-DE-3010.2S-5266-946-ND1-005</td>\n",
       "      <td>SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS_FWD PED...</td>\n",
       "      <td>SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS-FWD PED...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3171</th>\n",
       "      <td>I-ET-3010.2S-5266-970-ND1-001</td>\n",
       "      <td>I-ET-3010.2S-5266-970-ND1-001</td>\n",
       "      <td>INSPECTION AND TEST PLAN (ITP) PEDESTAL CRANE</td>\n",
       "      <td>INSPECTION AND TEST PLAN ITP PEDESTAL CRANE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               CODE                CLIENT_DOCUMENT  \\\n",
       "1     I-PR-3010.2S-1200-974-KD9-001  I-PR-3010.2S-1200-974-KD9-001   \n",
       "3     I-PR-3010.2S-1200-956-KD9-001  I-PR-3010.2S-1200-956-KD9-001   \n",
       "4     I-PR-3010.2S-1200-972-KD9-001  I-PR-3010.2S-1200-972-KD9-001   \n",
       "5     I-PR-3010.2S-1200-972-KD9-002  I-PR-3010.2S-1200-972-KD9-002   \n",
       "14    I-PR-3010.2S-1200-972-KD9-003  I-PR-3010.2S-1200-972-KD9-003   \n",
       "...                             ...                            ...   \n",
       "3162  I-DE-3010.2S-5266-946-ND1-001  I-DE-3010.2S-5266-946-ND1-001   \n",
       "3163  I-DE-3010.2S-5266-946-ND1-002  I-DE-3010.2S-5266-946-ND1-002   \n",
       "3165  I-DE-3010.2S-5266-946-ND1-004  I-DE-3010.2S-5266-946-ND1-004   \n",
       "3166  I-DE-3010.2S-5266-946-ND1-005  I-DE-3010.2S-5266-946-ND1-005   \n",
       "3171  I-ET-3010.2S-5266-970-ND1-001  I-ET-3010.2S-5266-970-ND1-001   \n",
       "\n",
       "                                         DOCUMENT_TITLE  \\\n",
       "1     PRESSURE TEST PROCEDURES -  API 610 & NON-API ...   \n",
       "3     SURFACE PREPARATION AND PAINTING PROCEDURE -  ...   \n",
       "4     PMI PROCEDURE - API 610 & NON-API CENTRIFUGAL ...   \n",
       "5     NON-DESTRUCTIVE EXAMINATION PROCEDURE - API 61...   \n",
       "14    HARDNESS PROCEDURE - API 610& NON-API  CENTRIF...   \n",
       "...                                                 ...   \n",
       "3162  INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...   \n",
       "3163  INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...   \n",
       "3165  SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS_AFT PED...   \n",
       "3166  SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS_FWD PED...   \n",
       "3171      INSPECTION AND TEST PLAN (ITP) PEDESTAL CRANE   \n",
       "\n",
       "                                                  TITLE  \n",
       "1     PRESSURE TEST PROCEDURES - API 610 AND NON-API...  \n",
       "3     SURFACE PREPARATION AND PAINTING PROCEDURE - A...  \n",
       "4     PMI PROCEDURE - API 610 AND NON-API CENTRIFUGA...  \n",
       "5     NON-DESTRUCTIVE EXAMINATION PROCEDURE - API 61...  \n",
       "14    HARDNESS PROCEDURE - API 610AND NON-API CENTRI...  \n",
       "...                                                 ...  \n",
       "3162  INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...  \n",
       "3163  INTERCONNECT DIAGRAM EXTERNAL CONNECTIONS FOR ...  \n",
       "3165  SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS-AFT PED...  \n",
       "3166  SINGLE LINE DIAGRAM ELECTRICAL SYSTEMS-FWD PED...  \n",
       "3171        INSPECTION AND TEST PLAN ITP PEDESTAL CRANE  \n",
       "\n",
       "[249 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\have_a_comment_mismatches.xlsx salvo em:  c:\\Users\\ELXY\\Documents\\Codigos\\Python\\P84_85\\LDs\n"
     ]
    }
   ],
   "source": [
    "# Create mask for non-matching titles\n",
    "mask = (df_LD_final['DOCUMENT_TITLE'] != df_LD_final['TITLE']) & (~df_LD_final['TITLE'].isna())\n",
    "\n",
    "# Get mismatched documents\n",
    "mismatches = df_LD_final[mask][['CODE', 'CLIENT_DOCUMENT', 'DOCUMENT_TITLE', 'TITLE']]\n",
    "\n",
    "print(\"Documents with mismatching titles:\")\n",
    "display(mismatches)\n",
    "\n",
    "# Salvar o DataFrame final\n",
    "output_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\have_a_comment_mismatches.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    mismatches.to_excel(writer, sheet_name='mismatches', index=False)\n",
    "\n",
    "print(f'Arquivo {output_file} salvo em: ', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar o DataFrame final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\LD_consolidada_have_a_comment.xlsx salvo em:  c:\\Users\\ELXY\\Documents\\Codigos\\Python\\P84_85\\LDs\n"
     ]
    }
   ],
   "source": [
    "# Salvar o DataFrame final\n",
    "output_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_LD_final.to_excel(writer, sheet_name='VDRL', index=False)\n",
    "\n",
    "print(f'Arquivo {output_file} salvo em: ', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega Have a Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "intput_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "\n",
    "df_LD_final = pd.read_excel(intput_file, sheet_name=\"VDRL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolida todas as VDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo existente C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\vda.xlsx foi excluído com sucesso.\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\All_VDA.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA AP5 14.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA ATI 19.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA D5A 65.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA D5A 66.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA FM2 36.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA GK1 41.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA HE1 21.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA HLT 06.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA HM9 26.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA KD9 33.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA KFQ 12.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA ND1 04.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA NZA 15.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA-106 Docs Sulzer.xlsx\n",
      "Arquivo consolidado salvo em: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\vda.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho da pasta contendo os arquivos VDA \n",
    "vda_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\'\n",
    "\n",
    "# Caminho do arquivo consolidado\n",
    "output_path = os.path.join(vda_path, 'vda.xlsx')\n",
    "\n",
    "# Verificar se o arquivo consolidado já existe e excluí-lo\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"Arquivo existente {output_path} foi excluído com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao excluir o arquivo {output_path}: {e}\")\n",
    "        print(\"O arquivo pode estar aberto em outro programa. Feche-o e tente novamente.\")\n",
    "        exit()  # Encerrar o script se não conseguir excluir o arquivo\n",
    "\n",
    "# Lista para armazenar todos os dataframes \n",
    "all_dfs = [] \n",
    "\n",
    "# Iterar sobre todos os arquivos Excel na pasta \n",
    "for filename in os.listdir(vda_path): \n",
    "    # Ignorar arquivos temporários/ocultos que começam com ~$ ou .\n",
    "    if filename.startswith('~$') or filename.startswith('.'):\n",
    "        print(f\"Ignorando arquivo temporário: {filename}\")\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.xlsx'): \n",
    "        file_path = os.path.join(vda_path, filename) \n",
    "        print(f\"Lendo arquivo: {file_path}\")\n",
    "        try:\n",
    "            # Especificar o engine como 'openpyxl' para arquivos .xlsx\n",
    "            df = pd.read_excel(file_path, engine='openpyxl') \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_path}: {e}\")\n",
    "            \n",
    "    elif filename.endswith('.xls'):\n",
    "        file_path = os.path.join(vda_path, filename) \n",
    "        print(f\"Lendo arquivo: {file_path}\")\n",
    "        try:\n",
    "            # Especificar o engine como 'xlrd' para arquivos .xls\n",
    "            df = pd.read_excel(file_path, engine='xlrd') \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_path}: {e}\")\n",
    "\n",
    "# Verificar se encontramos algum arquivo\n",
    "if all_dfs:\n",
    "    # Concatenar todos os dataframes \n",
    "    consolidated_df = pd.concat(all_dfs, ignore_index=True) \n",
    "    # Filter the dataframe to keep only system 1223 for HM9 trigram, and keep all data for other trigrams\n",
    "    consolidated_df = consolidated_df[\n",
    "        ((consolidated_df['Reference Document'].str.contains('-HM9-', na=False)) & \n",
    "        (consolidated_df['Reference Document'].str.contains('-1223-', na=False))) |\n",
    "        (~consolidated_df['Reference Document'].str.contains('-HM9-', na=False))\n",
    "]\n",
    "    # Salvar o dataframe consolidado e filtrado como um novo arquivo Excel \n",
    "    consolidated_df.to_excel(output_path, index=False) \n",
    "    print(f'Arquivo consolidado salvo em: {output_path}')\n",
    "else:\n",
    "    print(\"Nenhum arquivo Excel válido encontrado no diretório.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaciona o documento com a to-do-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo To Do List atualizado e salvo em: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\To Do List.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Caminhos dos arquivos \n",
    "todo_list_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\To Do List.xlsx'\n",
    "vda_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\vda.xlsx'\n",
    "\n",
    "# Leitura dos arquivos \n",
    "vda_df = pd.read_excel(vda_path)\n",
    "\n",
    "# Tentar ler o arquivo To Do List com diferentes métodos \n",
    "try:\n",
    "    # Primeiro, tente ler normalmente \n",
    "    todo_df = pd.read_excel(todo_list_path, engine='openpyxl')\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo normalmente: {e}\")\n",
    "    try:\n",
    "        # Se falhar, tente ler apenas os dados \n",
    "        wb = load_workbook(filename=todo_list_path, read_only=True, data_only=True)\n",
    "        ws = wb.active\n",
    "        data = ws.values\n",
    "        cols = next(data)[0:]\n",
    "        todo_df = pd.DataFrame(data, columns=cols)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo com openpyxl: {e}\")\n",
    "        # Se ambos falharem, crie um DataFrame vazio\n",
    "        todo_df = pd.DataFrame(columns=['Related Object'])\n",
    "\n",
    "# Verificar se a coluna 'Related Object' existe\n",
    "if 'Related Object' not in todo_df.columns:\n",
    "    print(\"A coluna 'Related Object' não foi encontrada. Adicionando uma coluna vazia.\")\n",
    "    todo_df['Related Object'] = ''\n",
    "\n",
    "# Resto do código permanece o mesmo \n",
    "# Merge do vda_df com df_LD_final \n",
    "merged_df = pd.merge(vda_df, df_LD_final, left_on='Reference Document', right_on='CLIENT_DOCUMENT', how='left')\n",
    "\n",
    "# Função para criar a string de informações \n",
    "def create_info_string(row):\n",
    "    if pd.notna(row['CLIENT_DOCUMENT']):\n",
    "        # Obter a data atual no formato desejado\n",
    "        current_date = datetime.now().strftime(\"%d-%B-%Y\")\n",
    "        # Limpar e padronizar o valor de Comments\n",
    "        have_comment = str(row['Comments']).strip().upper()\n",
    "\n",
    "        if have_comment == 'YES':\n",
    "            return f\"DOCUMENT: {row['CLIENT_DOCUMENT']} {row['Title']}, Comment: {row['Comments']}, Discipline: {row['Discipline']}, Date: {current_date}\"\n",
    "        else:\n",
    "            return f\"DOCUMENT: {row['CLIENT_DOCUMENT']} {row['Title']}, Comment: NO, Date: {current_date}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Aplicar a função para criar a coluna de informações \n",
    "merged_df['Info'] = merged_df.apply(create_info_string, axis=1)\n",
    "\n",
    "# Criar um dicionário de Code para Info\n",
    "code_to_info = dict(zip(merged_df['Code'], merged_df['Info']))\n",
    "\n",
    "# Função para preencher a coluna Additional Info\n",
    "def fill_additional_info(code):\n",
    "    return code_to_info.get(code, '')\n",
    "\n",
    "# Adicionar a nova coluna Additional Info no todo_df \n",
    "todo_df['Additional Info'] = todo_df['Related Object'].apply(fill_additional_info)\n",
    "\n",
    "# Update Additional Info based on Task Name condition\n",
    "todo_df.loc[todo_df['Task Name'] != 'VDA have a Comments', 'Additional Info'] = 'NÃO É HAVE A COMMENT'\n",
    "\n",
    "# Salvar o arquivo atualizado\n",
    "todo_df.to_excel(todo_list_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f'Arquivo To Do List atualizado e salvo em: {todo_list_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
