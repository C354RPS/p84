{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Have a Comments - Lista de Documentos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados Básicos (MESMO DO CÓDIGO controle.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dados_basicos = {\n",
    "'PKG_DESCRIPTION': ['API 610 CENTRIFUGAL PUMPS', 'DECK TROLLEY', 'FLARE SYSTEM', 'FRESH WATER CHLORINATION UNIT', 'NITROGEN GENERATOR UNITS', 'NON-API 610 CENTRIFUGAL PUMPS', 'PIG LAUNCHERS AND RECEIVERS', 'PROGRESSIVE CAVITY PUMPS', 'RECIPROCATING PUMPS', 'SEA WATER ELECTROCHLORINATION UNIT', 'SEA WATER LIFT PUMP', 'SHELL AND TUBE HEAT EXCHANGERS (Himile)', 'SHELL AND TUBE HEAT EXCHANGERS (Asvotec)', 'WATER INJECTION PUMPS', 'FRESH WATER MAKER FOR OIL DILUTION', 'PRINTED CIRCUIT HEAT EXCHANGER', 'OFFSHORE CRANE'],\n",
    "'Discipline': ['ROE', 'ROE', 'ROE', 'STATIC', 'ROE', 'ROE', 'STATIC', 'ROE', 'ROE', 'STATIC', 'ROE', 'STATIC', 'STATIC', 'ROE', 'STATIC', 'STATIC', 'ROE'],\n",
    "'Critical': ['', '', 'Critical', '', '', '', '', '', 'Critical', '', 'Super Critical', '', '', 'Super Critical', '', 'Critical', 'Critical'],\n",
    "'System': ['1200,1223,5124,5125,5331,5115', '5266', '5412', '5122', '5241', '120N,1200,5115,5124,5125', '1244,1231,1210', '5412', '5115,5133', '5121', '5111', '1223', '1200,1223,1251,1350,5125,5135,5147,1225,1231', '1251', '5122', '1231,1252,1254', '5266'],\n",
    "'Trigram': ['KD9', 'x', 'x', 'D5A', 'GK1', 'KD9', 'AP5', 'NZA', 'KFQ', 'D5A', 'FM2', 'HM9', 'ATI', 'SJA', 'HE1', 'HLT', 'ND1'],\n",
    "'Vendor': ['KSB BRASIL LTDA', '', '', 'DE NORA DO BRASIL LTDA', 'GARDNER DENVER KOREA', 'KSB BRASIL LTDA', 'APPLIED ENGINEERING PTE LTD', 'Netzsch Asia Pacific Pte Ltd', 'PERONI POMPE SPA', 'DE NORA DO BRASIL LTDA', 'FRAMO AS', 'HIMILE MECHANICAL MANUFACTURING (SHANDONG) CO., LTD', 'ASVOTEC TERMOINDUSTRIAL LTDA', 'SULZER BRASIL SA', 'HITACHI AQUA TECH (HAQT)E1', 'HEATRIC DIVISION OF MEGGITT (UK) LIMITED', 'NATIONAL OILWELL VARCO NORWAY AS'],\n",
    "'MR_Number': ['I-RM-3010.2S-1200-311-S2N-002', 'I-RM-3010.2S-5266-620-S2N-001', 'I-RM-3010.2S-5412-583-S2N-001', 'I-RM-3010.2S-5122-560-S2N-042', 'I-RM-3010.2S-5241-470-S2N-001', 'I-RM-3010.2S-1200-311-S2N-001', 'I-RM-3010.2S-1210-296-S2N-001', 'I-RM-3010.2S-1200-313-S2N-001', 'I-RM-3010.2S-5133-312-S2N-001', 'I-RM-3010.2S-5121-560-S2N-040', 'I-RM-3010.2S-5111-311-S2N-001', 'I-RM-3010.2S-1200-451-S2N-011', 'I-RM-3010.2S-1200-451-S2N-011', 'I-RM-3010.2S-1251-311-S2N-001', 'I-RM-3010.2S-5122-580-S2N-040', 'I-RM-3010.2S-1200-459-S2N-032', 'I-RM-3010.2S-5266-631-S2N-001'],\n",
    "'TBE_Number': ['I-PT-3010.2S-1200-311-S2N-002', 'I-PT-3010.2S-5266-620-S2N-001', 'I-PT-3010.2S-5412-583-S2N-001', 'I-PT-3010.2S-5122-560-S2N-042', 'I-PT-3010.2S-5241-470-S2N-001, I-PT-3010.2S-5241-470-S2N-002', 'I-PT-3010.2S-1200-311-S2N-001', 'I-PT-3010.2S-1210-296-S2N-018', 'I-PT-3010.2S-1200-313-S2N-001', 'I-PT-3010.2S-5133-312-S2N-001', 'I-PT-3010.2S-5121-560-S2N-040', 'I-PT-3010.2S-5111-311-S2N-001', 'I-PT-3010.2S-1200-451-S2N-011', 'I-PT-3010.2S-1200-451-S2N-011', 'I-PT-3010.2S-1251-311-S2N-001', 'I-PT-3010.2S-5122-580-S2N-040', 'I-PT-3010.2S-1200-459-S2N-032', 'I-PT-3010.2S-5266-631-S2N-001'],\n",
    "'KOM_DATE': ['', '', '', '10/03/2025', '18/02/2025', '', '24/03/2025', '', '', '07/03/2025', '17-18/02/2025', '21/03/2025', '19/03/2025', '21-22/10/2025', '25/03/2025', '','24/03/2025'],\n",
    "'PIM_DATE': ['', '', '', '', '01-02/04/2025', '', 's 08/2025', '', '', '', '', '', '', '17/01/2025', '', '', ''],\n",
    "'DR_MOTORS_DATE': ['', '', '', '', '', '', '', '', '', '', '', '', '', '17/03/2025', '', '',''],\n",
    "'PLAN_DELIVERY_DATE': ['', '', '', '28/02/2026', '30/10/2025 & 28/02/2026', '', '', '', '', '28/02/2026', '01/01/2027', '15/03/2026', '15/03/2026', '03/04/2026', '', '',''],\n",
    "}\n",
    "\n",
    "df_db = pd.DataFrame(data_dados_basicos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para extrair sistemas e trigramas do dataframe df_db (MESMO DO CÓDIGO controle.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Dicionário com os sistemas e trigramas\n",
      "sistemas_trigramas = [\n",
      "    ('1200', 'ATI'),\n",
      "    ('1200', 'KD9'),\n",
      "    ('120N', 'KD9'),\n",
      "    ('1210', 'AP5'),\n",
      "    ('1223', 'ATI'),\n",
      "    ('1223', 'HM9'),\n",
      "    ('1223', 'KD9'),\n",
      "    ('1225', 'ATI'),\n",
      "    ('1231', 'AP5'),\n",
      "    ('1231', 'ATI'),\n",
      "    ('1231', 'HLT'),\n",
      "    ('1244', 'AP5'),\n",
      "    ('1251', 'ATI'),\n",
      "    ('1251', 'SJA'),\n",
      "    ('1252', 'HLT'),\n",
      "    ('1254', 'HLT'),\n",
      "    ('1350', 'ATI'),\n",
      "    ('5111', 'FM2'),\n",
      "    ('5115', 'KD9'),\n",
      "    ('5115', 'KFQ'),\n",
      "    ('5121', 'D5A'),\n",
      "    ('5122', 'D5A'),\n",
      "    ('5122', 'HE1'),\n",
      "    ('5124', 'KD9'),\n",
      "    ('5125', 'ATI'),\n",
      "    ('5125', 'KD9'),\n",
      "    ('5133', 'KFQ'),\n",
      "    ('5135', 'ATI'),\n",
      "    ('5147', 'ATI'),\n",
      "    ('5241', 'GK1'),\n",
      "    ('5266', 'ND1'),\n",
      "    ('5266', 'x'),\n",
      "    ('5331', 'KD9'),\n",
      "    ('5412', 'NZA'),\n",
      "    ('5412', 'x'),\n",
      "    # Adicione mais sistemas e trigramas conforme necessário\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Função para extrair sistemas e trigramas do dataframe df_db\n",
    "\n",
    "def extrair_sistemas_trigramas(df):\n",
    "    # Inicializar um dicionário para armazenar relações sistema-trigrama\n",
    "    relacoes = {}\n",
    "    \n",
    "    # Iterar sobre cada linha do dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Obter o trigrama para esta linha\n",
    "        trigram = row['Trigram']\n",
    "        \n",
    "        # Limpar e dividir a string de sistemas\n",
    "        systems_str = row['System']\n",
    "        # Substituir espaços antes ou depois de vírgulas\n",
    "        systems_str = re.sub(r'\\s*,\\s*', ',', systems_str)\n",
    "        # Dividir a string pelos separadores de vírgula\n",
    "        systems = systems_str.split(',')\n",
    "        \n",
    "        # Para cada sistema nesta linha, associar ao trigrama\n",
    "        for system in systems:\n",
    "            system = system.strip()\n",
    "            if system:  # Verificar se o sistema não está vazio\n",
    "                if system not in relacoes:\n",
    "                    relacoes[system] = set()\n",
    "                relacoes[system].add(trigram)\n",
    "    \n",
    "    # Converter o dicionário para o formato de lista de tuplas\n",
    "    sistemas_trigramas = []\n",
    "    for system, trigrams in relacoes.items():\n",
    "        for trigram in trigrams:\n",
    "            sistemas_trigramas.append((system, trigram))\n",
    "    \n",
    "    # Ordenar a lista para melhor legibilidade\n",
    "    sistemas_trigramas.sort()\n",
    "    \n",
    "    return sistemas_trigramas\n",
    "\n",
    "\n",
    "# Extrair os sistemas e trigramas\n",
    "sistemas_trigramas = extrair_sistemas_trigramas(df_db)\n",
    "\n",
    "# Imprimir o resultado formatado como no exemplo\n",
    "print(\"# Dicionário com os sistemas e trigramas\")\n",
    "print(\"sistemas_trigramas = [\")\n",
    "for sistema, trigrama in sistemas_trigramas:\n",
    "    print(f\"    ('{sistema}', '{trigrama}'),\")\n",
    "print(\"    # Adicione mais sistemas e trigramas conforme necessário\")\n",
    "print(\"]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renomeia Colunas Duplicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renomear_colunas_revisao(df):\n",
    "    \"\"\"\n",
    "    Renomeia colunas duplicadas em um DataFrame usando prefixos de revisão (rev0_, revA_, revB_, etc.)\n",
    "    \n",
    "    Parâmetros:\n",
    "        df (pandas.DataFrame): DataFrame com possíveis nomes de colunas duplicados\n",
    "        \n",
    "    Retorna:\n",
    "        pandas.DataFrame: DataFrame com nomes de colunas únicos\n",
    "    \"\"\"\n",
    "    # Cria uma cópia do DataFrame para não modificar o original\n",
    "    df_renomeado = df.copy()\n",
    "    \n",
    "    # Identificando as colunas duplicadas e suas posições\n",
    "    posicoes = {}\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if col not in posicoes:\n",
    "            posicoes[col] = []\n",
    "        posicoes[col].append(i)\n",
    "    \n",
    "    # Lista de prefixos de revisão\n",
    "    prefixos_revisao = ['rev0_', 'revA_', 'revB_', 'revC_', 'revD_', 'revE_', \n",
    "                         'revF_', 'revG_', 'revH_', 'revI_', 'revJ_']\n",
    "    \n",
    "    # Criando novos nomes com os prefixos de revisão\n",
    "    novos_nomes = []\n",
    "    for i, col in enumerate(df.columns):\n",
    "        if len(posicoes[col]) > 1:\n",
    "            # Encontra o índice desta ocorrência na lista de posições\n",
    "            idx = posicoes[col].index(i)\n",
    "            if idx < len(prefixos_revisao):\n",
    "                novos_nomes.append(f'{prefixos_revisao[idx]}{col}')\n",
    "            else:\n",
    "                # Caso tenha mais revisões que prefixos disponíveis\n",
    "                novos_nomes.append(f'rev{idx}_{col}')\n",
    "        else:\n",
    "            # Colunas sem duplicatas mantêm o nome original\n",
    "            novos_nomes.append(col)\n",
    "    \n",
    "    # Atribui os novos nomes às colunas\n",
    "    df_renomeado.columns = novos_nomes\n",
    "    \n",
    "    return df_renomeado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 1200 e trigrama ATI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 1200 e trigrama KD9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 120N e trigrama KD9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 1210 e trigrama AP5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 1223 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1223 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 1223 e trigrama HM9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 1223 e trigrama KD9...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1223 e trigrama KD9\n",
      "Procurando/Processando arquivos para o sistema 1225 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1225 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 1231 e trigrama AP5...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1231 e trigrama AP5\n",
      "Procurando/Processando arquivos para o sistema 1231 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1231 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 1231 e trigrama HLT...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1231 e trigrama HLT\n",
      "Procurando/Processando arquivos para o sistema 1244 e trigrama AP5...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1244 e trigrama AP5\n",
      "Procurando/Processando arquivos para o sistema 1251 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1251 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 1251 e trigrama SJA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 1252 e trigrama HLT...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1252 e trigrama HLT\n",
      "Procurando/Processando arquivos para o sistema 1254 e trigrama HLT...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1254 e trigrama HLT\n",
      "Procurando/Processando arquivos para o sistema 1350 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 1350 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 5111 e trigrama FM2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 5115 e trigrama KD9...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5115 e trigrama KD9\n",
      "Procurando/Processando arquivos para o sistema 5115 e trigrama KFQ...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5115 e trigrama KFQ\n",
      "Procurando/Processando arquivos para o sistema 5121 e trigrama D5A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 5122 e trigrama D5A...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 5122 e trigrama HE1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 5124 e trigrama KD9...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5124 e trigrama KD9\n",
      "Procurando/Processando arquivos para o sistema 5125 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5125 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 5125 e trigrama KD9...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5125 e trigrama KD9\n",
      "Procurando/Processando arquivos para o sistema 5133 e trigrama KFQ...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5133 e trigrama KFQ\n",
      "Procurando/Processando arquivos para o sistema 5135 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5135 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 5147 e trigrama ATI...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5147 e trigrama ATI\n",
      "Procurando/Processando arquivos para o sistema 5241 e trigrama GK1...\n",
      "Procurando/Processando arquivos para o sistema 5266 e trigrama ND1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 5266 e trigrama x...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5266 e trigrama x\n",
      "Procurando/Processando arquivos para o sistema 5331 e trigrama KD9...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5331 e trigrama KD9\n",
      "Procurando/Processando arquivos para o sistema 5412 e trigrama NZA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "c:\\Users\\ELXY\\Documents\\Codigos\\Python\\.venv\\Lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Data Validation extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procurando/Processando arquivos para o sistema 5412 e trigrama x...\n",
      "Não foram encontrados pelo menos 2 arquivos para o sistema 5412 e trigrama x\n"
     ]
    }
   ],
   "source": [
    "# Caminho base para os arquivos\n",
    "base_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\files\\\\'\n",
    "\n",
    "# Arquivo do relatório Integra\n",
    "integra_report = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\RE-General Query - Technical Engineering Documents.xlsx'\n",
    "\n",
    "# Dataframe para armazenar todos os resultados\n",
    "df_LD_final = pd.DataFrame() \n",
    " \n",
    "# Processar cada sistema e trigrama \n",
    "for sistema, trigrama in sistemas_trigramas:\n",
    "    # Encontrar os arquivos correspondentes\n",
    "\n",
    "    print(f'Procurando/Processando arquivos para o sistema {sistema} e trigrama {trigrama}...')\n",
    "    # ld_files = glob.glob(f'{base_path}I-LD-3010.2S-{sistema}-*-{trigrama}-001*.xlsx')\n",
    "    ld_files = glob.glob(f'{base_path}I-LD-3010.2S-{sistema}-*-{trigrama}-*.xlsx')\n",
    "     \n",
    "    if len(ld_files) < 2:\n",
    "        print(f\"Não foram encontrados pelo menos 2 arquivos para o sistema {sistema} e trigrama {trigrama}\")\n",
    "        continue\n",
    "\n",
    "    ld_file = [f for f in ld_files if '_comments' not in f][0]\n",
    "    ld_file_comments = [f for f in ld_files if '_comments' in f][0]\n",
    "\n",
    "    # print(f'Processando arquivos: {ld_file} e {ld_file_comments}')\n",
    " \n",
    "    # Carregar os dataframes \n",
    "    df_LD = pd.read_excel(ld_file, sheet_name=\"VDRL\")\n",
    "    df_LD_comments = pd.read_excel(ld_file_comments, sheet_name=\"VDRL\")\n",
    "    df_integra = pd.read_excel(integra_report, sheet_name=\"DADOS\")\n",
    "\n",
    "    # Pré-processamento dos dados\n",
    "    df_LD.columns = df_LD.iloc[6]\n",
    "    df_LD = df_LD.iloc[7:]\n",
    " \n",
    "    df_LD_comments.columns = df_LD_comments.iloc[6]\n",
    "    df_LD_comments = df_LD_comments.iloc[7:]\n",
    " \n",
    "    df_LD = renomear_colunas_revisao(df_LD)\n",
    "    df_LD_comments = renomear_colunas_revisao(df_LD_comments)\n",
    "    \n",
    "    df_LD.rename(columns = {\"CLIENT DOCUMENT NUMBER\":\"CLIENT_DOCUMENT\",\n",
    "                            \"VENDOR DOCUMENT NUMBER\":\"VENDOR_DOCUMENT\",\n",
    "                            \"DOCUMENT TITLE\": \"DOCUMENT_TITLE\"}, inplace = True)\n",
    "    df_LD_comments.rename(columns = {\"Have comment?\":\"Have_comment\", \"CLIENT DOCUMENT NUMBER\":\"CLIENT_DOCUMENT\",\n",
    "                            \"VENDOR DOCUMENT NUMBER\":\"VENDOR_DOCUMENT\",\n",
    "                            \"DOCUMENT TITLE\": \"DOCUMENT_TITLE\"}, inplace = True)\n",
    "    \n",
    "    df_LD = df_LD.dropna(subset=['DOCUMENT_TITLE'])\n",
    "    df_LD_comments = df_LD_comments.dropna(subset=['DOCUMENT_TITLE'])\n",
    "    # Remover espaços em branco e converter para maiúsculas\n",
    "    df_LD['DOCUMENT_TITLE'] = df_LD['DOCUMENT_TITLE'].apply(lambda x: x.strip().upper())\n",
    "    df_LD_comments['DOCUMENT_TITLE'] = df_LD_comments['DOCUMENT_TITLE'].apply(lambda x: x.strip().upper())\n",
    " \n",
    "    df_integra.rename(columns = {\"CREATION DATE\":\"CREATION_DATE\"}, inplace = True)\n",
    "\n",
    "    colunas_para_manter = ['CLIENT_DOCUMENT', 'VENDOR_DOCUMENT', 'DOCUMENT_TITLE', 'ORIGINATOR ', 'rev0_PLANNED DATE','rev0_ACTUAL DATE','rev0_RETURNED DATE','rev0_RETURNED CODE',\n",
    "                            'revA_PLANNED DATE','revA_ACTUAL DATE','revA_RETURNED DATE','revA_RETURNED CODE', 'revB_PLANNED DATE','revB_ACTUAL DATE','revB_RETURNED DATE',\n",
    "                            'revB_RETURNED CODE']\n",
    "    df_LD.drop(columns=[col for col in df_LD.columns if col not in colunas_para_manter], inplace = True)\n",
    "    df_LD.reset_index(drop=True, inplace = True)\n",
    " \n",
    "    colunas_para_manter_VDA = ['Have_comment', 'Discipline', 'CLIENT_DOCUMENT', 'VENDOR_DOCUMENT', 'DOCUMENT_TITLE', 'ORIGINATOR ', 'rev0_PLANNED DATE','rev0_ACTUAL DATE',\n",
    "                               'rev0_RETURNED DATE','rev0_RETURNED CODE','revA_PLANNED DATE','revA_ACTUAL DATE','revA_RETURNED DATE','revA_RETURNED CODE', \n",
    "                               'revB_PLANNED DATE','revB_ACTUAL DATE','revB_RETURNED DATE','revB_RETURNED CODE']\n",
    "    df_LD_comments.drop(columns=[col for col in df_LD_comments.columns if col not in colunas_para_manter_VDA], inplace = True) \n",
    "    df_LD_comments.reset_index(drop=True, inplace = True) \n",
    " \n",
    "    df_LD = df_LD.iloc[:, :-2] \n",
    "    df_LD_comments = df_LD_comments.iloc[:, :-2] \n",
    "\n",
    "    # Processar os dados \n",
    "    for row in df_integra.itertuples(): \n",
    "        codigo = row.CODE \n",
    "        descricao = row.TITLE \n",
    "        dt_criacao = row.CREATION_DATE \n",
    "         \n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"CODE\"] = codigo \n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"TITLE\"] = descricao \n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"CREATION_DATE\"] = dt_criacao \n",
    " \n",
    "    for row in df_LD_comments.itertuples(): \n",
    "        have_comment = row.Have_comment \n",
    "        discipline = row.Discipline \n",
    "        codigo = row.CLIENT_DOCUMENT \n",
    "        descricao = row.DOCUMENT_TITLE \n",
    " \n",
    "        # avalia se tem o mesmo código de documento:\n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"CLIENT_DOCUMENT_cod\"] = codigo \n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"DOCUMENT_TITLE_cod\"] = descricao \n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"Have_comment_cod\"] = have_comment \n",
    "        df_LD.loc[(df_LD['CLIENT_DOCUMENT'] == codigo), \"Discipline_cod\"] = discipline\n",
    "        # avalia se tem a mesma descri;áo de documento:\n",
    "        df_LD.loc[(df_LD['DOCUMENT_TITLE'] == descricao), \"CLIENT_DOCUMENT_desc\"] = codigo \n",
    "        df_LD.loc[(df_LD['DOCUMENT_TITLE'] == descricao), \"DOCUMENT_TITLE_desc\"] = descricao \n",
    "        df_LD.loc[(df_LD['DOCUMENT_TITLE'] == descricao), \"Have_comment_desc\"] = have_comment \n",
    "        df_LD.loc[(df_LD['DOCUMENT_TITLE'] == descricao), \"Discipline_desc\"] = discipline \n",
    " \n",
    "    df_LD['Have_comment'] = np.where(df_LD['Have_comment_cod'].notnull(), \n",
    "                                     df_LD['Have_comment_cod'], \n",
    "                                     df_LD['Have_comment_desc']) \n",
    " \n",
    "    df_LD['Discipline'] = np.where(df_LD['Discipline_cod'].notnull(), \n",
    "                                     df_LD['Discipline_cod'], \n",
    "                                     df_LD['Discipline_desc']) \n",
    " \n",
    "    # Adicionar colunas de Sistema e Trigrama \n",
    "    df_LD['Sistema'] = sistema\n",
    "    df_LD['Trigrama'] = trigrama\n",
    "\n",
    "    # Adicionar ao dataframe final\n",
    "    df_LD_final = pd.concat([df_LD_final, df_LD], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cria uma coluna 'PLANNED DATE' com os valores mais atualizados da LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atualizar_planned_date_coalesce(df_LD):\n",
    "    \"\"\"\n",
    "    Versão usando coalesce para criar uma coluna 'PLANNED DATE' com os valores mais atualizados,\n",
    "    priorizando a revisão mais recente com valor válido.\n",
    "    \"\"\"\n",
    "    # Cria uma cópia do DataFrame\n",
    "    df = df_LD.copy()\n",
    "    \n",
    "    # Identifica as colunas de PLANNED DATE em ordem de prioridade\n",
    "    colunas_planned_date = [col for col in df.columns if 'PLANNED DATE' in col]\n",
    "    colunas_planned_date.sort(reverse=True)  # Coloca em ordem: revB, revA, rev0\n",
    "    \n",
    "    # Usa o método coalesce do pandas para pegar o primeiro valor não-NaN\n",
    "    df['PLANNED DATE'] = df[colunas_planned_date].bfill(axis=1).iloc[:, 0]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_LD_final = atualizar_planned_date_coalesce(df_LD_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvar o DataFrame final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\LD_consolidada_have_a_comment.xlsx salvo em:  c:\\Users\\ELXY\\Documents\\Codigos\\Python\\P84_85\\LDs\n"
     ]
    }
   ],
   "source": [
    "# Salvar o DataFrame final\n",
    "output_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file) as writer:\n",
    "    df_LD_final.to_excel(writer, sheet_name='LD', index=False)\n",
    "\n",
    "print(f'Arquivo {output_file} salvo em: ', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega Have a Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "intput_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "df_LD_final = pd.read_excel(intput_file, sheet_name=\"LD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolida todas as VDAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo existente C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\vda.xlsx foi excluído com sucesso.\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA ATI 01.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA D5A 31.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA FM2 12.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA GK1 18.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA HM9 04.xlsx\n",
      "Lendo arquivo: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\VDA-73 Docs Sulzer.xlsx\n",
      "Arquivo consolidado salvo em: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\VDA\\vda.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Caminho da pasta contendo os arquivos VDA \n",
    "vda_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\'\n",
    "\n",
    "# Caminho do arquivo consolidado\n",
    "output_path = os.path.join(vda_path, 'vda.xlsx')\n",
    "\n",
    "# Verificar se o arquivo consolidado já existe e excluí-lo\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        os.remove(output_path)\n",
    "        print(f\"Arquivo existente {output_path} foi excluído com sucesso.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao excluir o arquivo {output_path}: {e}\")\n",
    "        print(\"O arquivo pode estar aberto em outro programa. Feche-o e tente novamente.\")\n",
    "        exit()  # Encerrar o script se não conseguir excluir o arquivo\n",
    "\n",
    "# Lista para armazenar todos os dataframes \n",
    "all_dfs = [] \n",
    "\n",
    "# Iterar sobre todos os arquivos Excel na pasta \n",
    "for filename in os.listdir(vda_path): \n",
    "    # Ignorar arquivos temporários/ocultos que começam com ~$ ou .\n",
    "    if filename.startswith('~$') or filename.startswith('.'):\n",
    "        print(f\"Ignorando arquivo temporário: {filename}\")\n",
    "        continue\n",
    "        \n",
    "    if filename.endswith('.xlsx'): \n",
    "        file_path = os.path.join(vda_path, filename) \n",
    "        print(f\"Lendo arquivo: {file_path}\")\n",
    "        try:\n",
    "            # Especificar o engine como 'openpyxl' para arquivos .xlsx\n",
    "            df = pd.read_excel(file_path, engine='openpyxl') \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_path}: {e}\")\n",
    "            \n",
    "    elif filename.endswith('.xls'):\n",
    "        file_path = os.path.join(vda_path, filename) \n",
    "        print(f\"Lendo arquivo: {file_path}\")\n",
    "        try:\n",
    "            # Especificar o engine como 'xlrd' para arquivos .xls\n",
    "            df = pd.read_excel(file_path, engine='xlrd') \n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao ler {file_path}: {e}\")\n",
    "\n",
    "# Verificar se encontramos algum arquivo\n",
    "if all_dfs:\n",
    "    # Concatenar todos os dataframes \n",
    "    consolidated_df = pd.concat(all_dfs, ignore_index=True) \n",
    "    # Filter the dataframe to keep only system 1223 for HM9 trigram, and keep all data for other trigrams\n",
    "    consolidated_df = consolidated_df[\n",
    "        ((consolidated_df['Reference Document'].str.contains('-HM9-', na=False)) & \n",
    "        (consolidated_df['Reference Document'].str.contains('-1223-', na=False))) |\n",
    "        (~consolidated_df['Reference Document'].str.contains('-HM9-', na=False))\n",
    "]\n",
    "    # Salvar o dataframe consolidado e filtrado como um novo arquivo Excel \n",
    "    consolidated_df.to_excel(output_path, index=False) \n",
    "    print(f'Arquivo consolidado salvo em: {output_path}')\n",
    "else:\n",
    "    print(\"Nenhum arquivo Excel válido encontrado no diretório.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relaciona o documento com a to-do-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo To Do List atualizado e salvo em: C:\\Users\\elxy\\Documents\\Codigos\\Python\\P84_85\\LDs\\To Do List.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Caminhos dos arquivos \n",
    "todo_list_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\To Do List.xlsx'\n",
    "vda_path = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\vda.xlsx'\n",
    "\n",
    "# Leitura dos arquivos \n",
    "vda_df = pd.read_excel(vda_path)\n",
    "\n",
    "# Tentar ler o arquivo To Do List com diferentes métodos \n",
    "try: \n",
    "    # Primeiro, tente ler normalmente \n",
    "    todo_df = pd.read_excel(todo_list_path, engine='openpyxl')\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler o arquivo normalmente: {e}\")\n",
    "    try:\n",
    "        # Se falhar, tente ler apenas os dados \n",
    "        wb = load_workbook(filename=todo_list_path, read_only=True, data_only=True)\n",
    "        ws = wb.active \n",
    "        data = ws.values \n",
    "        cols = next(data)[0:] \n",
    "        todo_df = pd.DataFrame(data, columns=cols)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao ler o arquivo com openpyxl: {e}\")\n",
    "        # Se ambos falharem, crie um DataFrame vazio \n",
    "        todo_df = pd.DataFrame(columns=['Related Object'])\n",
    "\n",
    "# Verificar se a coluna 'Related Object' existe\n",
    "if 'Related Object' not in todo_df.columns: \n",
    "    print(\"A coluna 'Related Object' não foi encontrada. Adicionando uma coluna vazia.\")\n",
    "    todo_df['Related Object'] = ''\n",
    "\n",
    "# Resto do código permanece o mesmo \n",
    "# Merge do vda_df com df_LD_final \n",
    "merged_df = pd.merge(vda_df, df_LD_final, left_on='Reference Document', right_on='CLIENT_DOCUMENT', how='left')\n",
    "\n",
    "# Função para criar a string de informações \n",
    "def create_info_string(row):\n",
    "    if pd.notna(row['CLIENT_DOCUMENT']):\n",
    "        # Obter a data atual no formato desejado\n",
    "        current_date = datetime.now().strftime(\"%d-%B-%Y\")\n",
    "        # Limpar e padronizar o valor de Have_comment\n",
    "        have_comment = str(row['Have_comment']).strip().upper()\n",
    "\n",
    "        if have_comment == 'YES':\n",
    "            return f\"DOCUMENT: {row['CLIENT_DOCUMENT']} {row['Title']}, Comment: {row['Have_comment']}, Discipline: {row['Discipline']}, Date: {current_date}\"\n",
    "        else:\n",
    "            return f\"DOCUMENT: {row['CLIENT_DOCUMENT']} {row['Title']}, Comment: NO, Date: {current_date}\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Aplicar a função para criar a coluna de informações \n",
    "merged_df['Info'] = merged_df.apply(create_info_string, axis=1)\n",
    "\n",
    "# Criar um dicionário de Code para Info\n",
    "code_to_info = dict(zip(merged_df['Code'], merged_df['Info']))\n",
    "\n",
    "# Função para preencher a coluna Additional Info\n",
    "def fill_additional_info(code):\n",
    "    return code_to_info.get(code, '')\n",
    "\n",
    "# Adicionar a nova coluna Additional Info no todo_df \n",
    "todo_df['Additional Info'] = todo_df['Related Object'].apply(fill_additional_info)\n",
    "\n",
    "# Salvar o arquivo atualizado\n",
    "todo_df.to_excel(todo_list_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(f'Arquivo To Do List atualizado e salvo em: {todo_list_path}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
