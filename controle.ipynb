{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Powered System for Managing Vendor Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este projeto Python foi desenvolvido para auxiliar no gerenciamento e controle de pacotes de equipamentos dos FPSOs P84 e P85.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importa modulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignorando as mensagens de aviso (\"Deprecation Warning\") no python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"openpyxl.worksheet._reader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the correct path to the Excel file\n",
    "dados_basicos = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\DadosBasicos\\\\dados_basicos.xlsx'\n",
    "ld_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\LD_consolidada_have_a_comment.xlsx'\n",
    "mapa_topsideP84_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\P84 Procurement Map Topside.xlsx'\n",
    "mapa_topsideP85_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\P85 Procurement Map Topside.xlsx'\n",
    "mapa_orbis_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\orbis.xlsx'\n",
    "comunication_matrix_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\2024-03-10 - P84-P85 Package Communication Matrix.xlsx'\n",
    "integra_eng_docs = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\RE-General Query - Technical Engineering Documents.xlsx'\n",
    "vda_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\LDs\\\\VDA\\\\vda.xlsx'\n",
    "avanco_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\Avanço Pacotes.xlsx'\n",
    "notes_file = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\notes.xlsx'\n",
    "\n",
    "# Read the Excel file\n",
    "df_db = pd.read_excel(dados_basicos, sheet_name='DadosBasicos')\n",
    "df_all_equipment = pd.read_excel(dados_basicos, sheet_name='Equipamentos')\n",
    "df_LD = pd.read_excel(ld_file, sheet_name=\"VDRL\")\n",
    "df_mapa_topsideP84 = pd.read_excel(mapa_topsideP84_file, sheet_name=\"P84 Topside Map\")\n",
    "df_mapa_topsideP85 = pd.read_excel(mapa_topsideP85_file, sheet_name=\"P85 Topside Map\")\n",
    "df_mapa_orbis = pd.read_excel(mapa_orbis_file, sheet_name=\"Export\")\n",
    "df_mc = pd.read_excel(comunication_matrix_file, sheet_name=\"Package Matrix\")\n",
    "df_integra = pd.read_excel(integra_eng_docs, sheet_name=\"DADOS\")\n",
    "# cria um dataframe chamado df_integra_last somente com os documentos na última versão:\n",
    "df_integra_last = df_integra.loc[df_integra.groupby('CODE')['VERSION'].idxmax()]\n",
    "df_VDA = pd.read_excel(vda_file, sheet_name=\"Sheet1\")\n",
    "df_avanco = pd.read_excel(avanco_file, sheet_name=\"PREENCHIMENTO AVANÇO\")\n",
    "df_notes = pd.read_excel(notes_file, sheet_name=\"notes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pré-Processamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpeza de espaços em branco concluída para todos os DataFrames.\n",
      "Limpeza de espaços em branco concluída para todos os DataFrames (entre palavras).\n",
      "Ajustando o dataframe df_avanco para remover '- HULL' e - 'TOPSIDE.'\n"
     ]
    }
   ],
   "source": [
    "# Preparação dos DataFrames para limpeza de espaços em branco\n",
    "\n",
    "df_db = df_db.fillna('')\n",
    "df_mapa_orbis.rename(columns = {\"PKG DESCRIPTION\":\"PKG_DESCRIPTION\", \"Pkg ID\":\"Pkg_ID\", \"PO Issue Plan\":\"PO_Issue_Date_Plan\", \"Last deliv. Plan/Forecast\":\"Required_On_Site_Date_Plan\"}, inplace = True)\n",
    "df_mc.rename(columns={\"Package Engineer_Petrobras\":\"Package_Engineer_Petrobras\"}, inplace = True)\n",
    "df_mc.rename(columns={\"Package Engineer_Engineering\":\"Package_Engineer_Engineering\"}, inplace = True)\n",
    "df_mc.rename(columns={\"EPC_Responsible_(Expeditor)\":\"Package_Expeditor\"}, inplace = True)\n",
    "df_mc.rename(columns={\"Package Engineer_KBR\":\"Package_KBR\"}, inplace = True)\n",
    "df_avanco.rename(columns={\"PKG DESCRIPTION\":\"PKG_DESCRIPTION\", \"%PLAN\":\"PLAN\", \"%REAL\":\"REAL\", \"AVANÇO\":\"DESVIO\"}, inplace = True)\n",
    "df_avanco = df_avanco.drop(columns=['Unnamed: 6'])\n",
    "\n",
    "# limpeza de espaços em branco para todas as colunas de cada DataFrame da lista abaixo:\n",
    "# Lista de DataFrames\n",
    "dataframes = [df_LD, df_mapa_topsideP84, df_mapa_topsideP85, df_mapa_orbis, df_mc]\n",
    "\n",
    "# Função para limpar espaços em branco\n",
    "def limpar_espacos(df):\n",
    "    return df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "def limpar_espacos__entre_palavras(df):\n",
    "    return df.applymap(lambda x: re.sub(' +', ' ', str(x)) if isinstance(x, str) else x)\n",
    "\n",
    "# Aplicar a limpeza para cada DataFrame na lista\n",
    "for i, df in enumerate(dataframes):\n",
    "    dataframes[i] = limpar_espacos(df)\n",
    "\n",
    "# Atribuir os DataFrames limpos de volta às variáveis originais \n",
    "df_LD, df_mapa_topsideP84, df_mapa_topsideP85, df_mapa_orbis, df_mc = dataframes\n",
    " \n",
    "print(\"Limpeza de espaços em branco concluída para todos os DataFrames.\")\n",
    "\n",
    "# Aplicar a limpeza para cada DataFrame na lista\n",
    "for i, df in enumerate(dataframes):\n",
    "    dataframes[i] = limpar_espacos__entre_palavras(df)\n",
    " \n",
    "# Atribuir os DataFrames limpos de volta às variáveis originais \n",
    "df_LD, df_mapa_topsideP84, df_mapa_topsideP85, df_mapa_orbis, df_mc = dataframes\n",
    "\n",
    "print(\"Limpeza de espaços em branco concluída para todos os DataFrames (entre palavras).\")\n",
    "\n",
    "print(\"Ajustando o dataframe df_avanco para remover '- HULL' e - 'TOPSIDE.'\")\n",
    "df_avanco['PKG_DESCRIPTION'] = df_avanco['PKG_DESCRIPTION'].str.replace(' - HULL', '').str.replace(' - TOPSIDE', '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope of Supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de equipamentos do pacote API 610 CENTRIFUGAL PUMPS: 90\n",
      "Total de equipamentos do pacote DECK TROLLEY: 10\n",
      "Total de equipamentos do pacote FLARE SYSTEM: 8\n",
      "Total de equipamentos do pacote FRESH WATER CHLORINATION UNIT: 2\n",
      "Total de equipamentos do pacote FRESH WATER MAKER FOR OIL DILUTION: 6\n",
      "Total de equipamentos do pacote NITROGEN GENERATOR UNITS: 152\n",
      "Total de equipamentos do pacote NON-API 610 CENTRIFUGAL PUMPS: 17\n",
      "Total de equipamentos do pacote OFFSHORE CRANE: 4\n",
      "Total de equipamentos do pacote PIG LAUNCHERS AND RECEIVERS: 27\n",
      "Total de equipamentos do pacote PRINTED CIRCUIT HEAT EXCHANGER: 16\n",
      "Total de equipamentos do pacote PROGRESSIVE CAVITY PUMPS: 8\n",
      "Total de equipamentos do pacote RECIPROCATING PUMPS: 26\n",
      "Total de equipamentos do pacote SEA WATER ELECTROCHLORINATION UNIT: 2\n",
      "Total de equipamentos do pacote SEA WATER LIFT PUMP: 21\n",
      "Total de equipamentos do pacote SHELL AND TUBE HEAT EXCHANGERS (Asvotec): 27\n",
      "Total de equipamentos do pacote SHELL AND TUBE HEAT EXCHANGERS (Himile): 6\n",
      "Total de equipamentos do pacote WATER INJECTION PUMPS: 22\n",
      "\n",
      "Total de equipamentos: 444\n"
     ]
    }
   ],
   "source": [
    "def count_qty(df):\n",
    "    count = 0\n",
    "    for qty_str in df['QTY']:\n",
    "        parts = qty_str.split()\n",
    "        # Verificar se a primeira parte é um número válido\n",
    "        if parts[0].isdigit():\n",
    "            count += int(parts[0])\n",
    "\n",
    "    return count\n",
    "\n",
    "# Group df_all_equipment by PKG_DESCRIPTION\n",
    "df_grouped = df_all_equipment.groupby('PKG_DESCRIPTION')\n",
    "\n",
    "# Create empty list to store results\n",
    "data_for_df_equip = []\n",
    "\n",
    "total = 0\n",
    "for pkg_name, pkg_group in df_grouped:\n",
    "    # Apply count_qty function to each group\n",
    "    qty_count = count_qty(pkg_group)\n",
    "    \n",
    "    print(f\"Total de equipamentos do pacote {pkg_name}: {qty_count}\")\n",
    "    total += qty_count\n",
    "    \n",
    "    # Add data for the new DataFrame\n",
    "    data_for_df_equip.append({\n",
    "        'PKG_DESCRIPTION': pkg_name, \n",
    "        'QTY_EQUIPMENT': qty_count\n",
    "    })\n",
    "\n",
    "print(f\"\\nTotal de equipamentos: {total}\")\n",
    "\n",
    "# Create the new DataFrame df_equip\n",
    "df_equip = pd.DataFrame(data_for_df_equip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para extrair sistemas e trigramas do dataframe df_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Imprimir o resultado formatado como no exemplo\\nprint(\"# Dicionário com os sistemas e trigramas\")\\nprint(\"sistemas_trigramas = [\")\\nfor sistema, trigrama in sistemas_trigramas:\\n    print(f\"    (\\'{sistema}\\', \\'{trigrama}\\'),\")\\nprint(\"    # Adicione mais sistemas e trigramas conforme necessário\")\\nprint(\"]\")\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Função para extrair sistemas e trigramas do dataframe df_db\n",
    "\n",
    "def extrair_sistemas_trigramas(df):\n",
    "    # Inicializar um dicionário para armazenar relações sistema-trigrama\n",
    "    relacoes = {}\n",
    "    \n",
    "    # Iterar sobre cada linha do dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Obter o trigrama para esta linha\n",
    "        trigram = row['Trigram']\n",
    "        \n",
    "        # Limpar e dividir a string de sistemas\n",
    "        systems_str = row['System']\n",
    "        # Substituir espaços antes ou depois de vírgulas\n",
    "        systems_str = re.sub(r'\\s*,\\s*', ',', systems_str)\n",
    "        # Dividir a string pelos separadores de vírgula\n",
    "        systems = systems_str.split(',')\n",
    "        \n",
    "        # Para cada sistema nesta linha, associar ao trigrama\n",
    "        for system in systems:\n",
    "            system = system.strip()\n",
    "            if system:  # Verificar se o sistema não está vazio\n",
    "                if system not in relacoes:\n",
    "                    relacoes[system] = set()\n",
    "                relacoes[system].add(trigram)\n",
    "    \n",
    "    # Converter o dicionário para o formato de lista de tuplas\n",
    "    sistemas_trigramas = []\n",
    "    for system, trigrams in relacoes.items():\n",
    "        for trigram in trigrams:\n",
    "            sistemas_trigramas.append((system, trigram))\n",
    "    \n",
    "    # Ordenar a lista para melhor legibilidade\n",
    "    sistemas_trigramas.sort()\n",
    "    \n",
    "    return sistemas_trigramas\n",
    "\n",
    "\n",
    "# Extrair os sistemas e trigramas\n",
    "sistemas_trigramas = extrair_sistemas_trigramas(df_db)\n",
    "'''\n",
    "# Imprimir o resultado formatado como no exemplo\n",
    "print(\"# Dicionário com os sistemas e trigramas\")\n",
    "print(\"sistemas_trigramas = [\")\n",
    "for sistema, trigrama in sistemas_trigramas:\n",
    "    print(f\"    ('{sistema}', '{trigrama}'),\")\n",
    "print(\"    # Adicione mais sistemas e trigramas conforme necessário\")\n",
    "print(\"]\")\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAPA DE SUPRIMENTOS DO TOPSIDE:\n",
    "- Percorre os 2 dataframes e atualiza o df_escopo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando informações dos Mapas de Suprimentos...\n",
      "Processing DataFrame: df_mapa_topsideP84\n",
      "Processing DataFrame: df_mapa_topsideP85\n",
      "Atualizando informações do Orbis...\n",
      "Atualizando informações do Communication Matrix...\n",
      "Atualiza informações do Integra...\n",
      "Atualiza informações das quantidades de equipamentos...\n",
      "Atualiza informações do Avanço Geral...\n"
     ]
    }
   ],
   "source": [
    "lista_dfs = [\"df_mapa_topsideP84\", \"df_mapa_topsideP85\"]\n",
    "\n",
    "print(\"Atualizando informações dos Mapas de Suprimentos...\")\n",
    "for item in lista_dfs:\n",
    "    df = eval(item)  # Converte o nome da variável em um objeto DataFrame\n",
    "    print(f\"Processing DataFrame: {item}\")\n",
    "    \n",
    "    for row in df.itertuples():\n",
    "        pkg_description = row.PKG_DESCRIPTION\n",
    "        po_number = row.PO_Number\n",
    "        # usando a função .loc para atualizar os valores\n",
    "        if po_number:\n",
    "            df_db.loc[(df_db['PKG_DESCRIPTION'] == pkg_description), \"PO_Number\"] = po_number\n",
    "\n",
    "print(\"Atualizando informações do Orbis...\")\n",
    "for row in df_mapa_orbis.itertuples():\n",
    "    pkg_description = row.PKG_DESCRIPTION\n",
    "    pkg_ID = row.Pkg_ID\n",
    "    PO_Issue_Date_Plan = row.PO_Issue_Date_Plan\n",
    "    Required_On_Site_Date_Plan = row.Required_On_Site_Date_Plan\n",
    "    # if pkg_ID:\n",
    "    df_db.loc[(df_db['PKG_DESCRIPTION'] == pkg_description), \"PO_Issue_Date_Plan\"] = PO_Issue_Date_Plan\n",
    "    df_db.loc[(df_db['PKG_DESCRIPTION'] == pkg_description), \"Required_On_Site_Date_Plan\"] = Required_On_Site_Date_Plan\n",
    "\n",
    "print(\"Atualizando informações do Communication Matrix...\")\n",
    "for row in df_mc.itertuples():\n",
    "    pkg_description = row.PKG_DESCRIPTION\n",
    "    Package_KBR = row.Package_KBR\n",
    "    Package_Seatrium = row.Package_Engineer_Engineering\n",
    "    Package_Expeditor = row.Package_Expeditor\n",
    "    # if pkg_ID:\n",
    "    df_db.loc[(df_db['PKG_DESCRIPTION'] == pkg_description), \"Package_KBR\"] = Package_KBR\n",
    "    df_db.loc[(df_db['PKG_DESCRIPTION'] == pkg_description), \"Package_Seatrium\"] = Package_Seatrium\n",
    "    df_db.loc[(df_db['PKG_DESCRIPTION'] == pkg_description), \"Package_Expeditor\"] = Package_Expeditor\n",
    "\n",
    "print(\"Atualiza informações do Integra...\")\n",
    "\n",
    "for row in df_integra_last.itertuples():\n",
    "    document_emitido = row.CODE\n",
    "    \n",
    "    # Processa MR_Number que podem ter múltiplos valores\n",
    "    for index, mr_value in df_db['MR_Number'].items():\n",
    "        if pd.notna(mr_value):  # Verifica se o valor não é NaN\n",
    "            # Divide os valores separados por vírgula e remove espaços\n",
    "            mr_list = [item.strip() for item in str(mr_value).split(',')]\n",
    "            # Verifica se o documento emitido está na lista\n",
    "            if document_emitido in mr_list:\n",
    "                df_db.loc[index, \"MR_Emitida\"] = \"Sim\"\n",
    "    \n",
    "    # Processa TBE_Number que podem ter múltiplos valores\n",
    "    for index, tbe_value in df_db['TBE_Number'].items():\n",
    "        if pd.notna(tbe_value):  # Verifica se o valor não é NaN\n",
    "            # Divide os valores separados por vírgula e remove espaços\n",
    "            tbe_list = [item.strip() for item in str(tbe_value).split(',')]\n",
    "            # Verifica se o documento emitido está na lista\n",
    "            if document_emitido in tbe_list:\n",
    "                df_db.loc[index, \"TBE_Emitida\"] = \"Sim\"\n",
    "\n",
    "print(\"Atualiza informações das quantidades de equipamentos...\")\n",
    "# Update df_db with equipment quantities from df_equip\n",
    "for index, row in df_equip.iterrows():\n",
    "    pkg_description = row['PKG_DESCRIPTION']\n",
    "    qty_equipment = row['QTY_EQUIPMENT']\n",
    "    \n",
    "    # Update the corresponding rows in df_db\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'QTY_EQUIPMENT'] = qty_equipment\n",
    "\n",
    "print(\"Atualiza informações do Avanço Geral...\")\n",
    "# Substituir NaN por 0 nas colunas abaixo\n",
    "df_avanco['PLAN'] = df_avanco['PLAN'].fillna(0)\n",
    "df_avanco['REAL'] = df_avanco['REAL'].fillna(0)\n",
    "df_avanco['DESVIO'] = df_avanco['DESVIO'].fillna(0)\n",
    "\n",
    "# Filtra apenas os registros do FPSO P84\n",
    "df_avanco_p84 = df_avanco[df_avanco['FPSO'] == 'P84']\n",
    "# Ordena por data em ordem crescente para garantir que o último valor seja o mais recente\n",
    "df_avanco_p84 = df_avanco_p84.sort_values(by='DATE')\n",
    "# Obtém os últimos valores para cada PKG_DESCRIPTION\n",
    "df_latest_p84 = df_avanco_p84.drop_duplicates(subset=['PKG_DESCRIPTION'], keep='last')\n",
    "# Update df_db com os valores mais recentes\n",
    "for index, row in df_latest_p84.iterrows():\n",
    "    pkg_description = row['PKG_DESCRIPTION']\n",
    "    plan = row['PLAN']\n",
    "    real = row['REAL']\n",
    "    desvio = row['DESVIO']\n",
    "    # Update the corresponding rows in df_db\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'PLAN_P84'] = plan\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'REAL_P84'] = real\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'DESVIO_P84'] = desvio\n",
    "\n",
    "# Filtra apenas os registros do FPSO P84\n",
    "df_avanco_p85 = df_avanco[df_avanco['FPSO'] == 'P85']\n",
    "# Ordena por data em ordem crescente para garantir que o último valor seja o mais recente\n",
    "df_avanco_p85 = df_avanco_p85.sort_values(by='DATE')\n",
    "# Obtém os últimos valores para cada PKG_DESCRIPTION\n",
    "df_latest_p85 = df_avanco_p85.drop_duplicates(subset=['PKG_DESCRIPTION'], keep='last')\n",
    "# Update df_db com os valores mais recentes\n",
    "for index, row in df_latest_p85.iterrows():\n",
    "    pkg_description = row['PKG_DESCRIPTION']\n",
    "    plan = row['PLAN']\n",
    "    real = row['REAL']\n",
    "    desvio = row['DESVIO']\n",
    "    # Update the corresponding rows in df_db\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'PLAN_P85'] = plan\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'REAL_P85'] = real\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'DESVIO_P85'] = desvio\n",
    "\n",
    "# Substituir NaN por 0:\n",
    "df_db['PLAN_P85'] = df_db['PLAN_P85'].fillna(0)\n",
    "df_db['REAL_P85'] = df_db['REAL_P85'].fillna(0)\n",
    "df_db['DESVIO_P85'] = df_db['DESVIO_P85'].fillna(0)\n",
    "\n",
    "df_db['PLAN_P84'] = df_db['PLAN_P84'].fillna(0)\n",
    "df_db['REAL_P84'] = df_db['REAL_P84'].fillna(0)\n",
    "df_db['DESVIO_P84'] = df_db['DESVIO_P84'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avanço dos Pacotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentos em atraso com relação a última revisão da LD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Create a list to store delayed documents\n",
    "delayed_docs = []\n",
    "\n",
    "# Iterate through VDA documents\n",
    "for index, row in df_LD.iterrows():\n",
    "    client_document = row['CLIENT_DOCUMENT']\n",
    "    \n",
    "    # Verifica se PLANNED_DATE não é NaT\n",
    "    if pd.notna(row['PLANNED_DATE']):\n",
    "        planned_date = row['PLANNED_DATE'].date()\n",
    "        \n",
    "        # Check if document exists in df_integra_last\n",
    "        doc_issued = client_document in df_integra_last['CODE'].values\n",
    "        \n",
    "        # Document is delayed if not issued and planned date has passed\n",
    "        if not doc_issued and planned_date < today:\n",
    "            delayed_docs.append({\n",
    "                'CLIENT_DOCUMENT': client_document,\n",
    "                'DOCUMENT_TITLE': row['DOCUMENT_TITLE'],\n",
    "                'PLANNED_DATE': planned_date,\n",
    "                'DAYS_DELAYED': (today - planned_date).days,\n",
    "                'PKG_DESCRIPTION': row['PKG_DESCRIPTION']\n",
    "            })\n",
    "\n",
    "# Create DataFrame with delayed documents\n",
    "df_atrasados = pd.DataFrame(delayed_docs)\n",
    "\n",
    "# Se não houver documentos atrasados, crie um DataFrame vazio com as colunas necessárias\n",
    "if df_atrasados.empty:\n",
    "    df_atrasados = pd.DataFrame(columns=['CLIENT_DOCUMENT', 'DOCUMENT_TITLE', 'PLANNED_DATE', 'DAYS_DELAYED', 'PKG_DESCRIPTION'])\n",
    "\n",
    "# Count documents by PKG_DESCRIPTION\n",
    "if not df_atrasados.empty:\n",
    "    df_atrasados_pkg = df_atrasados['PKG_DESCRIPTION'].value_counts().reset_index()\n",
    "    df_atrasados_pkg.columns = ['PKG_DESCRIPTION', 'N_DOCS_EM_ATRASO']\n",
    "    \n",
    "    for index, row in df_atrasados_pkg.iterrows():\n",
    "        pkg_description = row['PKG_DESCRIPTION']\n",
    "        qty_docs = row['N_DOCS_EM_ATRASO']\n",
    "        # Update the corresponding rows in df_db\n",
    "        df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'N_DOCS_EM_ATRASO'] = qty_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a quantidade de documentos emitidos...\n"
     ]
    }
   ],
   "source": [
    "print(\"Atualizando a quantidade de documentos emitidos...\")\n",
    "# Create a mapping dictionary from df_LD\n",
    "pkg_map = df_LD.set_index('CLIENT_DOCUMENT')['PKG_DESCRIPTION'].to_dict()\n",
    "\n",
    "# Add new PKG_DESCRIPTION column to df_integra_last\n",
    "df_integra_last['PKG_DESCRIPTION'] = df_integra_last['CODE'].map(pkg_map)\n",
    "\n",
    "# Count documents by PKG_DESCRIPTION\n",
    "df_emitidos = df_integra_last['PKG_DESCRIPTION'].value_counts().reset_index()\n",
    "df_emitidos.columns = ['PKG_DESCRIPTION', 'N_DOCS_EMITIDOS']\n",
    "\n",
    "for index, row in df_emitidos.iterrows():\n",
    "    pkg_description = row['PKG_DESCRIPTION']\n",
    "    qty_docs = row['N_DOCS_EMITIDOS']\n",
    "    \n",
    "    # Update the corresponding rows in df_db\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'N_DOCS_EMITIDOS'] = qty_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atualizando a quantidade de documentos...\n",
      "Total de documentos original: 3180\n",
      "Total de documentos contados: 3180\n"
     ]
    }
   ],
   "source": [
    "print(\"Atualizando a quantidade de documentos...\")\n",
    "\n",
    "# Contagem mais robusta usando groupby e size\n",
    "df_num_docs = df_LD.groupby('PKG_DESCRIPTION', dropna=False).size().reset_index(name='count')\n",
    "\n",
    "# Update df_db with equipment quantities from df_num_docs\n",
    "for index, row in df_num_docs.iterrows():\n",
    "    pkg_description = row['PKG_DESCRIPTION']\n",
    "    qty_docs = row['count']\n",
    "    # Update the corresponding rows in df_db\n",
    "    df_db.loc[df_db['PKG_DESCRIPTION'] == pkg_description, 'N_DOCS'] = qty_docs\n",
    "\n",
    "# Verificar se todos os documentos foram contados\n",
    "total_docs_original = len(df_LD)\n",
    "total_docs_counted = df_num_docs['count'].sum()\n",
    "print(f\"Total de documentos original: {total_docs_original}\")\n",
    "print(f\"Total de documentos contados: {total_docs_counted}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comentário em Atraso: Seatrium vs Petrobras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import BusinessDay\n",
    "\n",
    "# Get today's date\n",
    "today = datetime.now().date()\n",
    "\n",
    "# Set different tolerance days for each company\n",
    "petrobras_tolerance_days = 8\n",
    "seatrium_tolerance_days = 7\n",
    "\n",
    "# Create lists to store delayed documents for each company\n",
    "seatrium_delays = []\n",
    "petrobras_delays = []\n",
    "\n",
    "# Function to calculate business days between two dates\n",
    "def business_days_between(start_date, end_date):\n",
    "    # Convert to datetime if they are not\n",
    "    if isinstance(start_date, str):\n",
    "        start_date = datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    if isinstance(end_date, str):\n",
    "        end_date = datetime.strptime(end_date, '%Y-%m-%d').date()\n",
    "    \n",
    "    # Calculate business days\n",
    "    business_days = np.busday_count(\n",
    "        start_date.strftime('%Y-%m-%d'),\n",
    "        end_date.strftime('%Y-%m-%d')\n",
    "    )\n",
    "    return business_days\n",
    "\n",
    "# Find the latest version for each document 'Code' in df_VDA\n",
    "df_VDA_latest = df_VDA.loc[df_VDA.groupby('Code')['Version'].idxmax()]\n",
    "\n",
    "# Check Petrobras delays (documents in VDA with status \"EM ANALISE\" for more than 8 business days)\n",
    "for _, row in df_VDA_latest.iterrows():\n",
    "    if row['Workflow Status'] == 'EM ANALISE':\n",
    "        creation_date = row['Creation Date'].date()\n",
    "        business_days = business_days_between(creation_date, today)\n",
    "        if business_days > petrobras_tolerance_days:\n",
    "            petrobras_delays.append({\n",
    "                'document': row['Reference Document'],\n",
    "                'vda': row['Code'],\n",
    "                'title': row['Title'],\n",
    "                'version': row['Version'],\n",
    "                'workflow_status': row['Workflow Status'],\n",
    "                'creation_date': row['Creation Date'],\n",
    "                'delay': business_days\n",
    "            })\n",
    "\n",
    "# Check Seatrium delays (documents in LD but not in VDA and created more than 7 business days ago)\n",
    "for _, row in df_LD.iterrows():\n",
    "    client_document = row['CLIENT_DOCUMENT']\n",
    "    # Check if document exists in df_integra_last\n",
    "    doc_issued = client_document in df_integra_last['CODE'].values\n",
    "    if doc_issued:\n",
    "        # Check if document is in VDA\n",
    "        doc_in_vda = client_document in df_VDA['Reference Document'].values\n",
    "        if not doc_in_vda:\n",
    "            # Get creation date from df_integra_last\n",
    "            creation_date = df_integra_last[df_integra_last['CODE'] == client_document]['CREATION DATE'].iloc[0].date()\n",
    "            business_days = business_days_between(creation_date, today)\n",
    "            if business_days > seatrium_tolerance_days:\n",
    "                seatrium_delays.append({\n",
    "                    'document': client_document,\n",
    "                    'title': row['TITLE'],\n",
    "                    'planned_date': row['PLANNED_DATE'],\n",
    "                    'discipline': row['Discipline'],\n",
    "                    'comment': row['Comments'],\n",
    "                    'creation_date': creation_date,\n",
    "                    'delay': business_days\n",
    "                })\n",
    "\n",
    "# Create DataFrames for delayed documents\n",
    "df_seatrium_delays = pd.DataFrame(seatrium_delays)\n",
    "df_petrobras_delays = pd.DataFrame(petrobras_delays)\n",
    "\n",
    "# Function to count delays for each package\n",
    "def count_package_delays(pkg_row, df_delays):\n",
    "    count = 0\n",
    "    trigrama = pkg_row['Trigram']\n",
    "    sistemas = pkg_row['System'].split(',') if pd.notna(pkg_row['System']) else []\n",
    "    sistemas = [s.strip() for s in sistemas]\n",
    "    for sistema in sistemas:\n",
    "        if pd.notna(trigrama) and pd.notna(sistema):\n",
    "            # Create regex pattern\n",
    "            pattern = f\".*{sistema}.*{trigrama}.*\"\n",
    "            # Count matching documents\n",
    "            matching_docs = df_delays[df_delays['document'].str.contains(pattern, regex=True, na=False)]\n",
    "            count += len(matching_docs)\n",
    "    return count\n",
    "\n",
    "# Mapeia coluna Discipline no df_petrobras_delays\n",
    "# Create a mapping dictionary from df_LD\n",
    "discipline_map = df_LD.set_index('CLIENT_DOCUMENT')['Discipline'].to_dict()\n",
    "# Add new Discipline column to df_petrobras_delays\n",
    "df_petrobras_delays['Discipline'] = df_petrobras_delays['document'].map(discipline_map)\n",
    "# Sort by Discipline and delay\n",
    "df_petrobras_delays = df_petrobras_delays.sort_values(['Discipline', 'delay'], ascending=[True, False])\n",
    "\n",
    "# Add new columns to df_db\n",
    "df_db['Atraso_KBR'] = df_db.apply(lambda row: count_package_delays(row, df_seatrium_delays), axis=1)\n",
    "df_db['Atraso_PB'] = df_db.apply(lambda row: count_package_delays(row, df_petrobras_delays), axis=1)\n",
    "\n",
    "# Definimos a nova ordem das colunas em uma lista\n",
    "new_order = ['PKG_DESCRIPTION',\t'QTY_EQUIPMENT', 'Discipline', 'Critical', 'System', 'Trigram', 'Vendor', \n",
    "             'MR_Number', 'MR_Emitida', 'TBE_Number', 'TBE_Emitida', 'PO_Number', 'KOM_DATE', 'PIM_DATE',\t\n",
    "             'DR_MOTORS_DATE', 'PLAN_DELIVERY_DATE', 'PO_Issue_Date_Plan', 'Required_On_Site_Date_Plan',\t\n",
    "             'Package_KBR',\t'Package_Seatrium',\t'Package_Expeditor', 'N_DOCS', 'N_DOCS_EMITIDOS', \n",
    "             'N_DOCS_EM_ATRASO', 'Atraso_KBR', 'Atraso_PB', 'PLAN_P84', 'REAL_P84', 'DESVIO_P84', 'PLAN_P85', 'REAL_P85', 'DESVIO_P85']\n",
    "\n",
    "\n",
    "# Replace NaN with 0 for specific columns\n",
    "columns_to_fill = ['N_DOCS', 'N_DOCS_EMITIDOS', 'N_DOCS_EM_ATRASO']\n",
    "df_db[columns_to_fill] = df_db[columns_to_fill].fillna(0)\n",
    "\n",
    "# Usamos essa lista para reordenar o DataFrame\n",
    "df_db = df_db[new_order]\n",
    "\n",
    "# Display updated df_db\n",
    "# display(df_db)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Salva os dados em Arquivo Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 2025-06-27 P84-P85_controle_cesar.xlsx salvo em:  C:\\Users\\ELXY\\Documents\\Codigos\\Python\\P84_85\n"
     ]
    }
   ],
   "source": [
    "# control_cesar = 'C:\\\\Users\\\\elxy\\\\Documents\\\\Codigos\\\\Python\\\\P84_85\\\\2025-05-28_P84-P85_controle_cesar.xlsx'\n",
    "from datetime import datetime\n",
    "\n",
    "data_atual = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "control_cesar = f\"{data_atual} P84-P85_controle_cesar.xlsx\"\n",
    "\n",
    "# Salvando arquivos P84 e P85:\n",
    "with pd.ExcelWriter(control_cesar) as writer:\n",
    "    df_integra.to_excel(writer, sheet_name='integra_all_eng_docs', index=False)\n",
    "    df_db.to_excel(writer, sheet_name='Resumo', index=False)\n",
    "    df_all_equipment.to_excel(writer, sheet_name='equip', index=False)\n",
    "    df_atrasados.to_excel(writer, sheet_name='docs_atrasados', index=False)\n",
    "    df_petrobras_delays.to_excel(writer, sheet_name='petrobras_delays', index=False)\n",
    "    df_seatrium_delays.to_excel(writer, sheet_name='seatrium_delays', index=False)\n",
    "print(f'Arquivo {control_cesar} salvo em: ', Path.cwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRÁFICOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import pandas as pd\n",
    "\n",
    "# Inicializar lista de figuras para armazenar os gráficos\n",
    "figures = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plota Gráfico com Número de Documentos e Documentos em Atraso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do estilo do Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Definir o tamanho da figura\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Preparar os dados para plotagem\n",
    "# Transformar os dados em formato long (melhor para Seaborn)\n",
    "df_long = pd.melt(df_db,\n",
    "                  id_vars=['PKG_DESCRIPTION'],\n",
    "                  value_vars=['N_DOCS', 'N_DOCS_EMITIDOS', 'N_DOCS_EM_ATRASO'],\n",
    "                  var_name='Tipo',\n",
    "                  value_name='Número de Documentos')\n",
    "\n",
    "# Criar o gráfico de barras lado a lado\n",
    "sns.barplot(x='PKG_DESCRIPTION', \n",
    "            y='Número de Documentos', \n",
    "            hue='Tipo', \n",
    "            data=df_long,\n",
    "            palette={'N_DOCS': 'blue', 'N_DOCS_EMITIDOS': 'green', 'N_DOCS_EM_ATRASO': 'red'})\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Package Description')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Number of Documents by Package', fontdict={\"size\": 16})\n",
    "\n",
    "# Rotacionar os rótulos do eixo x\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i in range(len(df_db['PKG_DESCRIPTION'])):\n",
    "    for j, col in enumerate(['N_DOCS', 'N_DOCS_EMITIDOS', 'N_DOCS_EM_ATRASO']):\n",
    "        valor = df_db[col][i]\n",
    "        if valor != 0:\n",
    "            plt.text(i + (j * 0.4) - 0.4, \n",
    "                     valor, \n",
    "                     str(valor), \n",
    "                     ha='center', \n",
    "                     va='bottom')\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Capturar a figura (se necessário)\n",
    "figures.append(plt.gcf())\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plota Gráfico dos Comentários em Atraso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do estilo do Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Definir o tamanho da figura\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Usar uma paleta de cores predefinida do Seaborn\n",
    "custom_palette = {\n",
    "    'Atraso_KBR': mcolors.to_rgba('#1A5F7A', alpha=0.7),  # Azul com transparência\n",
    "    'Atraso_PB': mcolors.to_rgba(\"#FA700E\", alpha=0.7)    # Marrom com transparência\n",
    "}\n",
    "\n",
    "# Preparar os dados para plotagem\n",
    "# Transformar os dados em formato long (melhor para Seaborn)\n",
    "df_long = pd.melt(df_db, \n",
    "                  id_vars=['PKG_DESCRIPTION'], \n",
    "                  value_vars=['Atraso_KBR', 'Atraso_PB'],\n",
    "                  var_name='Tipo',\n",
    "                  value_name='Documentos em Atraso')\n",
    "\n",
    "# Criar o gráfico de barras lado a lado usando a paleta de cores\n",
    "sns.barplot(x='PKG_DESCRIPTION', \n",
    "            y='Documentos em Atraso', \n",
    "            hue='Tipo', \n",
    "            data=df_long,\n",
    "            # palette={'Atraso_KBR': 'blue', 'Atraso_PB': 'orange'})\n",
    "            palette=custom_palette)\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Package Description')\n",
    "plt.ylabel('Overdue Documents')\n",
    "plt.title('Overdue Comments by Package', fontdict={\"size\": 16})\n",
    "\n",
    "# Rotacionar os rótulos do eixo x\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i in range(len(df_db['PKG_DESCRIPTION'])):\n",
    "    for j, col in enumerate(['Atraso_KBR', 'Atraso_PB']):\n",
    "        valor = df_db[col][i]\n",
    "        if valor != 0:\n",
    "            plt.text(i + (j * 0.4) - 0.2, \n",
    "                     valor, \n",
    "                     str(valor), \n",
    "                     ha='center', \n",
    "                     va='bottom')\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Capturar a figura\n",
    "figures.append(plt.gcf())\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plota Gráfico do Avanço Geral por Pacote - P84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do estilo do Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Definir o tamanho da figura\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Preparar os dados para plotagem\n",
    "# Transformar os dados em formato long (melhor para Seaborn)\n",
    "df_long = pd.melt(df_db, \n",
    "                  id_vars=['PKG_DESCRIPTION'], \n",
    "                  value_vars=['PLAN_P84', 'REAL_P84'], \n",
    "                  var_name='Tipo', \n",
    "                  value_name='Progresso')\n",
    "\n",
    "# Criar o gráfico de barras lado a lado\n",
    "sns.barplot(x='PKG_DESCRIPTION', \n",
    "            y='Progresso', \n",
    "            hue='Tipo', \n",
    "            data=df_long,\n",
    "            palette={'PLAN_P84': 'blue', 'REAL_P84': 'green'})\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Package Description')\n",
    "plt.ylabel('Progress (%)')\n",
    "plt.title('Overall Progress by Package - P84', fontdict={\"size\": 16})\n",
    "\n",
    "# Rotacionar os rótulos do eixo x\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i in range(len(df_db['PKG_DESCRIPTION'])):\n",
    "    for j, col in enumerate(['PLAN_P84', 'REAL_P84']):\n",
    "        valor = df_db[col][i]\n",
    "        if valor != 0:\n",
    "            plt.text(i + (j * 0.4) - 0.2, \n",
    "                     valor, \n",
    "                     str(valor), \n",
    "                     ha='center', \n",
    "                     va='bottom')\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Capturar a figura (se necessário)\n",
    "figures.append(plt.gcf())\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plota Gráfico do Avanço Geral por Pacote - P85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração do estilo do Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Definir o tamanho da figura\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Preparar os dados para plotagem\n",
    "# Transformar os dados em formato long (melhor para Seaborn)\n",
    "df_long = pd.melt(df_db, \n",
    "                  id_vars=['PKG_DESCRIPTION'], \n",
    "                  value_vars=['PLAN_P85', 'REAL_P85'], \n",
    "                  var_name='Tipo', \n",
    "                  value_name='Progresso')\n",
    "\n",
    "# Criar o gráfico de barras lado a lado\n",
    "sns.barplot(x='PKG_DESCRIPTION', \n",
    "            y='Progresso', \n",
    "            hue='Tipo', \n",
    "            data=df_long,\n",
    "            palette={'PLAN_P85': 'blue', 'REAL_P85': 'green'})\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "plt.xlabel('Package Description')\n",
    "plt.ylabel('Progress (%)')\n",
    "plt.title('Overall Progress by Package - P85', fontdict={\"size\": 16})\n",
    "\n",
    "# Rotacionar os rótulos do eixo x\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for i in range(len(df_db['PKG_DESCRIPTION'])):\n",
    "    for j, col in enumerate(['PLAN_P85', 'REAL_P85']):\n",
    "        valor = df_db[col][i]\n",
    "        if valor != 0:\n",
    "            plt.text(i + (j * 0.4) - 0.2, \n",
    "                     valor, \n",
    "                     str(valor), \n",
    "                     ha='center', \n",
    "                     va='bottom')\n",
    "\n",
    "# Ajustar o layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Capturar a figura (se necessário)\n",
    "figures.append(plt.gcf())\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gera relatório de avanço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "from reportlab.lib import colors\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle\n",
    "from reportlab.lib.units import inch, cm\n",
    "\n",
    "def generate_report(df_notes, figures, output_filename):\n",
    "    \"\"\"\n",
    "    Gera um relatório PDF baseado em um dataframe de notas e figuras.\n",
    "    \n",
    "    Args:\n",
    "    df_notes (pandas.DataFrame): DataFrame contendo as notas para o relatório \n",
    "    com colunas: 'PKG_DESCRIPTION', 'DATE', 'Tipo', 'Descricao'\n",
    "    figures (list): Lista de objetos de figura (matplotlib ou caminho para imagens)\n",
    "    output_filename (str): Nome do arquivo PDF de saída\n",
    "    \"\"\"\n",
    "    \n",
    "    # Configurar o documento em tamanho A4 com margens estreitas\n",
    "    page_width, page_height = A4\n",
    "    margin = 1.0 * cm # 1 cm de margem em todos os lados\n",
    "    \n",
    "    doc = SimpleDocTemplate(\n",
    "        output_filename,\n",
    "        pagesize=A4,\n",
    "        rightMargin=margin,\n",
    "        leftMargin=margin,\n",
    "        topMargin=margin,\n",
    "        bottomMargin=margin\n",
    "    )\n",
    "    \n",
    "    styles = getSampleStyleSheet()\n",
    "    \n",
    "    # Calcular largura disponível para conteúdo\n",
    "    content_width = page_width - (2 * margin)\n",
    "    \n",
    "    # Criar estilos personalizados com nomes únicos\n",
    "    title_style = ParagraphStyle(\n",
    "        name='CustomTitle',\n",
    "        parent=styles['Heading1'],\n",
    "        fontSize=16,\n",
    "        alignment=1, # centralizado\n",
    "        spaceAfter=12\n",
    "    )\n",
    "    \n",
    "    subtitle_style = ParagraphStyle(\n",
    "        name='CustomSubtitle',\n",
    "        parent=styles['Heading2'],\n",
    "        fontSize=12,\n",
    "        alignment=1, # centralizado\n",
    "        spaceAfter=12\n",
    "    )\n",
    "    \n",
    "    section_title_style = ParagraphStyle(\n",
    "        name='CustomSectionTitle',\n",
    "        parent=styles['Heading2'],\n",
    "        fontSize=14,\n",
    "        spaceAfter=6\n",
    "    )\n",
    "    \n",
    "    package_title_style = ParagraphStyle(\n",
    "        name='CustomPackageTitle',\n",
    "        parent=styles['Heading3'],\n",
    "        fontSize=11,\n",
    "        fontName='Helvetica-Bold',\n",
    "        spaceAfter=4,\n",
    "        spaceBefore=10\n",
    "    )\n",
    "    \n",
    "    bullet_point_style = ParagraphStyle(\n",
    "        name='CustomBulletPoint',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=10,\n",
    "        leftIndent=20,\n",
    "        bulletIndent=10,\n",
    "        spaceAfter=2\n",
    "    )\n",
    "    \n",
    "    sub_header_style = ParagraphStyle(\n",
    "        name='CustomSubHeader',\n",
    "        parent=styles['Normal'],\n",
    "        fontSize=11,\n",
    "        fontName='Helvetica-Bold',\n",
    "        spaceAfter=2,\n",
    "        spaceBefore=6\n",
    "    )\n",
    "    \n",
    "    # Conteúdo do documento\n",
    "    elements = []\n",
    "    \n",
    "    # Título\n",
    "    elements.append(Paragraph(\"Progress Report - CÉSAR SILVESTRE\", title_style))\n",
    "    \n",
    "    # Data\n",
    "    current_date = datetime.datetime.now().strftime(\"%d/%m/%Y\")\n",
    "    elements.append(Paragraph(f\"Date: {current_date}\", subtitle_style))\n",
    "    elements.append(Spacer(1, 0.25*inch))\n",
    "    \n",
    "    # Seção 1: Indicadores\n",
    "    elements.append(Paragraph(\"1. Metrics:\", section_title_style))\n",
    "    elements.append(Spacer(1, 0.1*inch))\n",
    "    \n",
    "    # Adicionar as figuras uma embaixo da outra\n",
    "    if len(figures) > 0:\n",
    "        # Salvar temporariamente as figuras matplotlib se necessário\n",
    "        image_paths = []\n",
    "        for i, fig in enumerate(figures):\n",
    "            if hasattr(fig, 'savefig'): # Se for uma figura matplotlib\n",
    "                temp_path = f'temp_figure_{i}.png'\n",
    "                fig.savefig(temp_path, dpi=100, bbox_inches='tight')\n",
    "                image_paths.append(temp_path)\n",
    "            else: # Se for um caminho para uma imagem\n",
    "                image_paths.append(fig)\n",
    "        \n",
    "        # Definir dimensões para aproveitar melhor o espaço A4\n",
    "        fig_width = content_width # Usa toda a largura disponível\n",
    "        fig_height = content_width * 0.55 # Proporção aproximada para gráficos\n",
    "        \n",
    "        # Adicionar cada figura individualmente\n",
    "        for img_path in image_paths:\n",
    "            img = Image(img_path, width=fig_width, height=fig_height)\n",
    "            elements.append(img)\n",
    "            elements.append(Spacer(1, 0.2*inch)) # Espaço entre figuras\n",
    "        \n",
    "        elements.append(Spacer(1, 0.3*inch))\n",
    "    \n",
    "    # Seção 2: Comentários\n",
    "    elements.append(Paragraph(\"2. Comments:\", section_title_style))\n",
    "    elements.append(Spacer(1, 0.1*inch))\n",
    "    \n",
    "    # Agrupar por pacote\n",
    "    grouped_notes = df_notes.groupby('PKG_DESCRIPTION')\n",
    "    \n",
    "    # Processar cada pacote\n",
    "    for pkg_description, pkg_group in grouped_notes:\n",
    "        elements.append(Paragraph(pkg_description, package_title_style))\n",
    "        \n",
    "        # Processar Fatos Relevantes\n",
    "        relevant_facts = pkg_group[pkg_group['Tipo'] == 'Fato Relevante']\n",
    "        if not relevant_facts.empty:\n",
    "            elements.append(Paragraph(\"<b><i>Relevant Facts:</i></b>\", sub_header_style))\n",
    "            for _, row in relevant_facts.iterrows():\n",
    "                elements.append(Paragraph(f\"{row['Descricao']}\", bullet_point_style))\n",
    "        \n",
    "        # Processar Pontos de Atenção\n",
    "        attention_points = pkg_group[pkg_group['Tipo'] == 'Ponto de Atenção']\n",
    "        if not attention_points.empty:\n",
    "            elements.append(Paragraph(\"<b><i>Concerns:</i></b>\", sub_header_style))\n",
    "            for _, row in attention_points.iterrows():\n",
    "                elements.append(Paragraph(f\"{row['Descricao']}\", bullet_point_style))\n",
    "        \n",
    "        elements.append(Spacer(1, 0.1*inch))\n",
    "    \n",
    "    # Gerar o PDF\n",
    "    doc.build(elements)\n",
    "    print(f\"Relatório gerado com sucesso: {output_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gera Relatórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando relatório gerencial...\n",
      "Nome do relatório: 2025-06-27 Relatório de Avanço - César.pdf\n",
      "Relatório gerado com sucesso: 2025-06-27 Relatório de Avanço - César.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Gerando relatório gerencial...\")\n",
    "\n",
    "data_atual = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "nome_relatorio = f\"{data_atual} Relatório de Avanço - César.pdf\"\n",
    "print(f\"Nome do relatório: {nome_relatorio}\")\n",
    "# Gere o relatório\n",
    "generate_report(df_notes, figures, nome_relatorio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
